# **第12章 其它存储技术**





## PowerPath功能概览

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-06-01*

PowerPath是基于主机的用于智能地管理多路径I/O的软件。PowerPath可实现多路径、自动故障切换以及动态负载均衡，可用于管理Symmetrix，Clariion，以及第三方存储阵列。



本文主要介绍EMC多路径控制软件PowerPath的功能。





**什么是PowerPath:**



PowerPath是基于主机的用于智能地管理多路径I/O的软件。PowerPath可实现多路径、自动故障切换以及动态负载均衡，可用于管理Symmetrix，Clariion，以及第三方存储阵列。

 

路径（Path）指的是在主机与存储系统逻辑单元（Logical Unit, LU）之间的物理路线。包括主机总线适配器（HBA）端口，电缆，交换机，存储系统接口和端口，以及LU。LU指可作为单一存储卷被寻址的物理或虚拟设备。对于iSCSI标准，路径指Initiator-Target-LUN。

 

PowerPath支持对一个逻辑设备的多路径连接，使用PowerPath可提供以下功能：

- 硬件故障发生时自动故障切换。PowerPath自动检测到路径故障并将I/O重定向至另一条路径。
- 动态多路径负载均衡。PowerPath将对一个逻辑设备的I/O请求分布于所有可用路径，因此提升了I/O性能并减少了管理时间，并由于无需在逻辑设备之间静态配置路径而减少了故障停机时间。

 



**多路径功能：**

**
**

PowerPath可通过多个端口连接到逻辑设备。用户可使用两个或更多接口将逻辑设备配置为共享设备。通过此方式，所有逻辑设备在所有端口可见，从而提升了可用性。

 

如下图所示，没有PowerPath的情况下，主机的SCSI驱动将无法通过多路径连接到一个逻辑设备。这是由于大多数操作系统将一条路径看作一个独特的逻辑设备，尽管在多条路径连接到同一逻辑设备的情况下也是如此。这会导致系统crash时数据的丢失。而PowerPath排除了这样的限制。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWMglVdBZJrWia1IPgH1icsO8tJo2bZ6wvCfibnnm0HlnzcIFzenJLhgIictsUKNag6WNAhs4UfibdFEsw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

使用PowerPath的情况下，用户可以通过多条路径连接到逻辑设备从而实现主机和存储端口的共享。使用Fabric配置的共享路径数量会更多。例如，主机具有4个HBA通过Fabric连接到主机的4个端口100个逻辑设备上，PowerPath管理1600条路径。（4HBAs X 4Fas x 100 logical devices = 1600）。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWMglVdBZJrWia1IPgH1icsO858Fj1M3ZC8bFe7oiaMN4CxzRlcDwVDb45CWv23oRejOalSdl3C87Zrg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如上图所示，两个逻辑设备都可通过两个接口端口访问，从而逻辑设备的I/O可在多条路径上分流。上图中，两条路径连接到逻辑设备0同时两条连接到逻辑设备1.

 

PowerPath利用了存储系统的多路径性能，在主机和逻辑设备间提供负载均衡或防路径故障的功能。从而PowerPath能够：

- 通过在多条路径发送I/O请求到同一逻辑设备增加I/O吞吐量。
- 通过将I/O请求从一条故障路径重定向至另一工作路径防止数据丢失。

 



**动态负载均衡:**

**
**

PowerPath通过动态负载均衡在维持最大性能的同时降低管理成本，它的设计目的是在所有时间使用所有路径。PowerPath将对一个逻辑设备的I/O分布在所有可用路径上，而不是让一条路径承担所有的I/O负荷。（对于active-passive存储系统，每一个逻辑设备的可用路径指的是那些连接到active SP的路径。）

 

PowerPath在host-by-host基础上对I/O负载均衡，它对于所有路径维护所有I/O的统计数据。对于每一个I/O请求，PowerPath根据实施的负载均衡和故障切换策略智能地选择负担最小的可用路径。如果策略正确，PowerPah系统中所有路径都会有近似相同的负载。

 

除了改进I/O性能之外，动态负载均衡减少了管理时间和故障停机时间，因为管理员不再需要在逻辑设备之间静态配置路径。使用PowerPath，不需要安装时间，路径一直保持按照性能优化的方式来配置。

 

下图是没有安装PowerPath时的I/O队列：

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWMglVdBZJrWia1IPgH1icsO89MoJ3ASALpSgnD9zapkgxfJNvMJ2W10dT4DUE8LGwnLQy8pNo00k0A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



下图是使用了PowerPath之后的I/O负载状况：



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWMglVdBZJrWia1IPgH1icsO8yvMeeGzzq1qjeYBX8lLPLgd1r7ZkA7gTlZsuSgTvaUnPicuvYqnBEpg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**自动故障切换:**



下图说明了I/O路径的故障点：

- HBA/NIC
- Interconnect（Cable和Patch Panel）
- Switch
- Interface
- Interface port

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWMglVdBZJrWia1IPgH1icsO8qMeDDpJiboP9dUCK8H2lEly6OdT9p1R3cxLYTpNPrpeGd9rpMKlwf8g/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

如果发生路径故障，PowerPath将该路径上的I/O重新分配到正常工作的路径。PowerPath停止向故障路径发送I/O检查可用路径。如果没有可用路径，则将替代或备用路径投入使用，I/O导入替代路径。

 

PowerPath使用周期性路径测试以确认路径是否能够正常工作。路径测试是PowerPath通过发送一系列I/O以确认路径的可用性。如果测试失败，PowerPath关闭该路径并停止向其发送I/O。

 

PowerPath继续周期性地检测故障路径，以确认其是否恢复。如果路径通过测试，PowerPath将恢复对该路径的使用并重新发送I/O。在轻量负载或小型配置的情况下，路径在修复后会在一小时内自动恢复使用。对于大型配置，修复后恢复所有路径使用可能花费数小时，因为周期性自动恢复任务被更高优先级任务抢占。路径的故障切换以及恢复流程对于应用程序来说是透明的。当路径恢复后，存储，主机，应用程序将继续保持可用性。

 

测试正常工作路径将花费几毫秒，测试故障路径可能花费数秒，具体取决于故障类型。















## 软件定义存储概念解析

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-03-30*

   2012年，VMware在其vForum大会上首次提出软件定义数据中心(SDDC)的概念。作为VMware软件定义数据中心五大组成部分(计算、存储、网络、管理和安全)之一，软件定义存储(SDS)的概念也首次被提出。EMC公司在当年的EMC World发布大会上也发布了软件定义存储战略，引发了业界对软件定义存储的大讨论，软件定义存储迅速成为存储业界的研究热点。本文将介绍软件定义存储的基本概念。

 

 

**软件定义存储的定义**

 

软件定义存储(SDS)是一种简单的智能化存储数据中心体系结构，它采用的是动态、敏捷的自动化解决方案，摒弃了静态、专门构建的低效硬件，从而能够满足用户的业务和应用需求。软件定义的存储(SDS) 通过使用虚拟数据层对底层存储进行抽象化，这使得虚拟机、应用和存储基本单元能够跨异构存储系统进行调配和管理。通过对应用和可用资源进行灵活的隔离处理，虚拟化管理程序可对应用所需的所有IT 资源实施负载平衡，包括计算、内存、存储和网络连接资源。全球网络存储工业协会（SNIA）认为软件定义存储应该包括如下功能：

 

- 自动化
- 标准接口
- 虚拟数据路径
- 扩展性
- 透明性

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUCcRXTLDHG91cHrVvicJq7veDnTJapItC835NDtfJ9t4ueRjuxTnZAnDiaIm5Ma73XibdLlcEe8WXrw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



  软件定义存储的解决方案使当前的数据中心更为强大，因为它可提供：

 

- 应用级存储服务：SDS在虚拟机级别实施，这使得存储服务能够根据应用的要求量身定制，同时还能根据每个应用的需要进行调整，而不会对相邻的应用造成影响。管理员可全面控制每个应用使用的存储服务，因此也能控制成本。

 

- 快速更改存储基础架构：SDS采用的是动态无中断的模式，就像计算虚拟化一样。IT管理员可以精确地满足应用需求并及时提供所需的资源。存储服务变得十分灵活，比如：当前给这个应用多分一点，随后给那个应用少分一点。

 

- 异构存储支持：SDS可提供十足的灵活性，使您能够充分利用现有的存储解决方案，例如SAN、NAS或者是基于x86业界标准硬件的直连存储。借助商用服务器硬件（即超融合基础架构的主干），IT组织可以设计低成本、可扩展的存储环境，轻松适应特定的和不断变化的存储需求。















## Flash中的Flash

[戴尔易安信技术支持](javascript:void(0);) *2016-06-03*

https://v.qq.com/x/page/a0196tzhsh1.html

Flash的飞速发展正在全面改变存储行业的游戏规则。CPU每10年性能提升100倍，机械硬盘的转速却没有多大的变化。所以现在性能依靠闪存盘，提高容量的使命由SATA盘和大容量盘来完成。 EMC今年推出了面向服务器及存储系统的EMC Xtrem系列闪存优化产品：服务器本地存储XtremSF硬件、服务器闪存缓存XtremSW Cache软件、全闪阵列XtremIO和混合阵列四个方面的产品和方案。 Xtrem系列闪存产品体现了EMC对闪存市场软硬结合的战略：硬件提供多重选择，软件实现关键的功能。















## 论存储IOPS和Throughput吞吐量之间的关系

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-06-12*

   IOPS和Throughput吞吐量两个参数是衡量存储性能的主要指标。IOPS表示存储每秒传输IO的数量，Throughput吞吐量则表示每秒数据的传输总量。两者在不同的情况下都能表示存储的性能状况，但应用的场景不尽相同。同时，两者之间也存在着相互的联系，本文就IOPS和Throughput吞吐量对存储性能衡量的场景入手，描述两者之间的变化关系与计算方法。帮助读者更好的了解存储的性能分析与规划。

 

**IOPS与Throughput的关系:**

 

   IOPS（IO per Second）是用来计算I/O流中每个节点中每秒传输的数量（关于IO流中的每个节点的解释，参考文章：  

[浅析I/O处理过程与存储性能的关系](http://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651770990&idx=1&sn=5492710235cdbb75fc07817145c51a78&scene=21#wechat_redirect)）。通常情况下，广义的IOPS指得是服务器和存储系统处理的I/O数量。但是，由于在IO传输的过程中，数据包会被分割成多块（block），交由存储阵列缓存或者磁盘处理，对于磁盘来说这样每个block在存储系统内部也被视为一个I/O，存储系统内部由缓存到磁盘的的数据处理也会以IOPS来作为计量的指标之一。本文中提到的IOPS，是指得广义的IOPS，即由服务器发起的，并由存储系统中处理的I/O单位。



   IOPS通常对于小I/O，且传输I/O的数量比较大的情况下，是一个最主要的衡量指标。例如，典型的OLTP系统中，高的IOPS则意味着数据库的事务可以被存储系统处理。



   Throughput吞吐量是用来计算每秒在I/O流中传输的数据总量。这个指标，在大多数的磁盘性能计算工具中都会显示，最简单的在Windows文件拷贝的时候，就会显示MB/s。通常情况下，Throughput吞吐量只会计算I/O包中的数据部分，至于I/O包头的数据则会被忽略在Throughput吞吐量的计算中。广义上的Throughput吞吐量，也会被叫做“带宽”，用来衡量I/O流中的传输通道，比如2/4/8Gbps Fibre Channel、60Mbps SCSI等等。但 “带宽”会包括通道中所有数据的总传输量的最大值，而Throughput吞吐量则是只保护传输的实际数据，两者还是有些许区别。



   Throughput吞吐量衡量对于大I/O，特别是传输一定数据的时候最小化耗时非常有用。备份数据的时候是一个典型的例子。在备份作业中，我们通常不会关心有多少I/O被存储系统处理了，而是完成备份总数据的时间多少。



   IOPS和Throughput吞吐量之间存在着线性的变化关系，而决定它们的变化的变量就是每个I/O的大小。从图中可以看到，当被传输的I/O比较小的情况下，每个I/O所需传输的时间会比较少，单位时间内传输的I/O数量就多。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWYic4OUw50MP0iaaD65RraK7ASz8Br1tjcR9jYrVSc9GQz1UzcF6HN1smsagN0ZIHyA8wkZ3bGVFPg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

而由于处理数据包头，总的时间内传输实际数据相对较低。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWYic4OUw50MP0iaaD65RraK7buiaKunsuQcTX6SI3quZDbduuwIWsvoOFzUV1pmOxk87YlFJVvmUk0w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

当I/O尺寸比较大的情况下，如下图所示，传输每个I/O的时间增大，IOPS数量下降。但是相比更高的百分比的I/O通道用来传输实际数据，Throughput则明显上升。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWYic4OUw50MP0iaaD65RraK7EbB3dVSTcbTUTlOV9Lwl9HWaMvl9KcXCxhHBULDdNRtLbwk0AjaG0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

我们可以用一个简单的公式来计算Throughput和IOPS之间的关系：

 

Throughput MB/s = IOPS * KB per IO / 1024

 

   假设一个10个10K SAS磁盘，每个磁盘提供140 IOPS，总共有1400最大IOPS。理论上这些磁盘处理不同的IO大小，所能达到的Throughput吞吐量是有区别的。简单的来说，物理层面IOPS和Throughput哪个先达到了物理磁盘的极限，就决定了这个物理磁盘的性能阀值。下面的计算公式可以看到，单位I/O大小可以使得吞吐量成倍提升，但是未能达到10个SAS磁盘1GB/S（每个磁盘100MB/s带宽）的理论“带宽”。显而易见，因为大多数应用的I/O不会那么大，所以你会看到存储阵列的吞吐量远小于厂商提供的理论值，原因就是因为IOPS先达到了性能阀值，使得吞吐量无法再提升。当然也有特殊的应用，例如流媒体服务器等，应用端可以使用2MB的I/O大小，那么吞吐量利用率显然会更加高，IOPS的要求则相对较低了。

 

MB/s = 1400 * 64 /1024 = 87.5 MB/s

MB/s = 1400 * 128 /1024 = 175 MB/s

MB/s = 1400 * 256 /1024 = 350 MB/s

 

   综上所述，在规划存储性能和处理存储性能问题的时候，需要综合看IOPS和Throughput吞吐量这两个参数，本文的观点总结为以下几点：

 

1. 性能工具统计的Throughput吞吐量永远达不到实际的I/O流中节点的理论“带宽”，原因是性能工具不会统计I/O的包头信息，而是实际的数据传输量。
2. 磁盘物理层面IOPS和Throughput哪个先达到了物理磁盘的极限，就决定了这个物理磁盘的性能阀值，然而决定哪个先达到性能阀值的就是I/O的大小。
3. 性能监控工具显示IOPS低或者Throughput低于预期，先不要直接认为存储性能存在问题，搞清楚应用的I/O大小，再做后续判断。
4. 存储性能另外一个重要因素还有磁盘响应时间（Response Time），本文的内容是建立在存储可以提供接受访问内的响应时间为前提。

















## 浅析闪存盘（Flash Drive）内部架构与应用考虑

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-08-04*

   Flash Drive闪存盘，作为代替使用了几十年的机械硬盘，它与机械硬盘相比有着绝对的性能优势，架构上也与机械硬盘有着很大的区别。本文以摘要的形式梳理了闪存盘的内部结构Page、Block、Channel的作用以及使用闪存盘上的一些性能考虑和使用范围。

 

**闪存盘就像一个小型的存储系统:**

 

闪存盘就像一个小型的存储系统，它包括一下的组件：

- **缓存区（Buffer）**： 缓存里存着所有区域的索引信息，正在写入的数据、并且使用专用的电容器（Power capacitors）在系统掉电的时候为缓存供电，然后把数据写入到持久存储NAND单元。
- **页面（Pages）**：闪存盘内部的芯片通过页面为单位来寻址，73GB和200GB的闪存盘页面大小为4KB，400GB的闪存盘页面大小16KB。页面内容组成连续的寻址空间，就像存储系统中的缓存一样。写入过程中，对于两个2KB的IO在闪存页面中必须写入两个连续的LBA（Logical Block Address），也就是占用两个页面。
- **扇区（Blocks）**：NAND类型的闪存盘的映射关系和文件系统类似，多个Pages组成为Block，但是区域中的页面并不是连续的，这个block和SCSI以及文件系统中的block还是有所区别。NAND类型的闪存盘写入都是在Block级别。Block的镜像存储在闪存盘的缓存中，知道一个区域写满了，然后再写入到闪存盘存储单元的Block内。
- **通道（Channels）**：闪存盘中芯片处理数据的通路，闪存盘拥有多个通道，保证存储单元可以同时进行读和写的操作，对于大IO，会在多个通道上分段处理。

 

闪存盘中的页面通过几种状态来管理数据状态：

- Valid State：包含可用数据
- Invalid State: 包含过期数据
- Erased State：没有被使用区域

 

   页面状态会受到负载的影响，页面状态也会影响到可用的区域。可用的区域（已经释放的区域）会决定写入的性能。由于随机写入页面变得随机分布，闪存盘使用元数据来定位Valid和Invalid的数据，比如一个占两个Block文件更新写入，第一个block写入到缓存，那么它所对应的NAND上的两个block都会变成invalid。



   闪存盘中还有一部预留空间，存储通常意义上的元数据，预留空间会为写入数据提供可用的block位置。



   持续的数据写入会使闪存盘饱和，然后闪存盘会在空闲的时间进行擦除（Erase）操作。在擦除之前，为了保证Block中的页面必须都是invalid状态，会将每个block中的valid页面写入到其他block，这个过程类似于硬盘碎片整理，主要分为几个步骤：

- 将Valid的页面读取到缓存中
- 擦除NAND中旧的区域
- 将Valid页面写入NAND的到其他Block中

 

 

**使用闪存盘时的一些性能考虑与应用场景:**

 

   是否闪存盘在使用久了以后会变慢？答案是，的确会有一些影响。闪存盘的剩余空间会是主要因素之一，因为剩余空间的减少会导致碎片的增加，影响到闪存盘对于持续写入的相应时间，较高的空间利用率会导致在每个block中有更多的valid页面。随着时间增加，这种valid页面的分布会变得更加随机分布而且空间利用率增加。如果block中包含比较高的比例的valid页面，擦除的过程中就需要调整更多的页面。闪存盘就需要更多的时间进行碎片整理。



   闪存盘的大小是否影响性能？这里需要描述一个概念，叫做小IO和写入页面填充的概念，因为在闪存盘处理写入IO的情况中，如果写入的IO小于一个页面的大小，则闪存盘需要进行Read-Modify-Write操作。因此对于闪存盘来说的负载加倍。所以就闪存盘性能来说，73GB和200GB使用的4KB页面大小会好于400GB的16KB页面大小。



   最后还要简要提一下闪存盘的应用场景。其实处于成本效益的考虑，闪存盘的使用还是需要有的放矢。根据适合的应用选择，而且结合存储阵列的情况进行综合考虑，作者认为可以从以下几个方面考虑：

- 闪存盘通常还适用于高比例的随机读取和小IO的应用。
- 如果随机写入的比例大于20%，则是闪存盘最好发挥作用的情况。
- 闪存盘和存储阵列的缓存相结合。闪存盘很多情况下会加速存储阵列的数据刷新效率，比较典型的就是高负债写入的应用，存储阵列的缓存可以提供更短的前端响应时间，减少从存储阵列缓存中载入和写入后端磁盘速度。但当某些极大的数据库情况下也有可能相反，存储阵列缓存对于大量的写入可能会响应不及，而这种情况，根据使用存储阵列实际情况，可以选择性的跳过存储阵列的写缓存，而直接在后端使用大量（超过30个闪存盘）提供高带宽，当然也要考虑存储阵列是否支持关闭写缓存。
- 对于数据库应用，闪存盘适用于存放索引、比较繁忙的表、临时空间，而Redo Log和Archive log之类则不适合于存储在闪存盘。
- 对于消息系统（Exchange、Notes），闪存盘适用于存放用户数据库。























## 智能存储系统概念解析

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-08-05*

智能存储系统是配置了多块硬盘和大量内存，并提供多条I/O通路，拥有智能操作系统的存储阵列。它采用复杂的算法来实现最优化的存储资源处理，以满足高性能需求的应用程序。本文将介绍智能存储系统的组成部件。

 

一般来说，前端、缓存、后端和物理磁盘四部分构成一个智能存储系统。智能存储系统的数据处理是为从主机发起的I/O请求到达前端端口开始，然后经过缓存。如果被请求的数据保存在缓存中，那么读请求直接在缓存中完成。如果数据不在缓存中，则经过后端端口到物理磁盘上读取数据。主机被写入数据经过前端端口，先写入缓存，然后经后端端口保存到物理磁盘。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWPM8EuDp1Qqgo8b6VamDIVhWRl8tibHxsjTRUZhq3OLiahO0V7T33QXwArLH7164evbaXvtYafd8rw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

 

**前端**

 

智能存储系统的前端由前端端口和前端控制器组成，它提供了存储系统和主机之间高可用的连接端口。一个智能存储系统拥有多个前端端口，以实现大量主机同时对智能存储系统的读写。不同的前端端口支持不同的传输协议，这些传输协议包括FC、FCoE和iSCSI等。前端控制器通过内部数据总线将数据写入缓存货从缓存读取数据。

 

 

**缓存**

 

缓存可以通俗理解为内存，它可以减少主机I/O访问请求所需时间。缓存的最小单位是页，缓存就是由页组成。应用I/O的大小决定了缓存页的大小。由于缓存没有寻道时间和旋转延时，因此缓存的响应时间不到机械磁盘的十分之一，可以快速的处理大量数据或者实时数据，大幅提升了智能存储系统的性能。

 

**后端**

 

智能存储系统的后端由后端端口和后端控制器组成，它提供了缓存和物理磁盘的连接端口。一个智能存储系统可以拥有多个后端端口，这样可以提供更好的数据保护、负载均衡能力和冗余性。数据从缓存通过后端端口传到物理磁盘，后端控制器为读写数据提供临时的数据存储，并充当交流着的角色。

 

**物理磁盘**

 

物理磁盘的主要作用是保存数据。智能存储支持不同类型的物理磁盘，如：闪存盘、SATA盘和SAS盘等。一般来说，闪存盘数据访问时间最快，SATA盘数据访问时间最慢，用户可以根据不同应用类型配置不同类型的物理磁盘。

















## 存储技术基本概念

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-07-30*

本文为存储概念基础贴。内容涵盖存储技术从NAS, SAN到主机，交换机，网络，接口协议，阵列等一系列概念。帮助存储爱好者扫清学习过程中的知识盲点。



 

**网络附加存储（Network Attached Storage，NAS）:**

 

是一种专门的数据存储技术的名称，它可以直接连接在计算机网络上面，对异质网络用户提供了集中式数据访问服务。



NAS和传统的文件存储服务或直接存储设备不同的地方，在于NAS设备上面的操作系统和软件只提供了数据存储、数据访问、以及相关的管理功能；此外，NAS设备也提供了不止一种文件传输协议。NAS系统通常有一个以上的硬盘，而且和传统的文件服务器一样，通常会把它们组成RAID来提供服务；有了NAS以后，网络上的其他服务器就可以不必再兼任文件服务器的功能。NAS的型式很多样化，可以是一个大量生产的嵌入式设备，也可以在一般的计算机上运行NAS的软件。



NAS用的是以文件为单位的通信协议，例如像是NFS（在UNIX系统上很常见）或是SMB（常用于Windows系统）。NAS所用的是以文件为单位的通信协议，大家都很清楚它们的运作模式，相对之下，存储区域网络（SAN）用的则是以区块为单位的通信协议、通常是通过SCSI再转为光纤通道或是iSCSI。（还有其他各种不同的SAN通信协议，像是ATA over Ethernet和HyperSCSI，不过这些都不常见。）



NAS计算机或设备用的通常是精简版的操作系统，只提供了最单纯的文件服务和其相关的通信协议；举例来说，有一个叫FreeNAS的开放源代码NAS软件用的就是精简版的FreeBSD，它可以在一般的计算机硬件上运行，而商业化的嵌入式设备用的则是封闭源码的操作系统和通信协议程序。

 



**存储区域网络（Storage Area Network, SAN）:**

 

是一种连接外接存储设备和服务器的架构。人们采用包括光纤通道技术、磁盘阵列、磁带柜、光盘柜（en）的各种技术进行实现。该架构的特点是，连接到服务器的存储设备，将被操作系统视为直接连接的存储设备。尽管SAN的复杂度和价格已经下降，但目前在大型企业级存储方案以外还应用不甚广泛。



与SAN相比较，网络储存设备（NAS, Network Attached Storage）使用的是基于文件的通信协议，例如NFS或SMB/CIFS通信协议就被明确的定义为远程存储设备，计算机请求访问的是抽象文件的一段内容，而非对磁盘进行的块设备操作。

 



**HBA:**

 

我们知道网卡是用于连接计算机和计算机网络。网卡一般插在计算机大总线扩展槽上，卡上有连接计算机网络的接口。网卡物理上连接计算机内部总线，例如PCI,PCI-X,PCI-E,SUN的Sbus总线等，和计算机网络，例如以太网等。存储系统中也有类似的用于连接计算机内部总线和存储网络的设备。这种位于服务器上与存储网络连接的设备一般称为主机总线适配卡（Host Bus Adaptor）HBA。HBA是服务器内部的I/O通道与存储系统的I/O通道之间的物理连接。最常用的服务器内部I/O通道是PCI和Sbus，它们是连接服务器CPU和外围设备的通讯协议。存储系统的I/O通道实际上就是光纤通道。而HBA的作用就是实现内部通道协议PCI或Sbus和光纤通道协议之间的转换。



常见的服务器和存储设备之间的数据通讯协议是IDE，SCSI和光纤通道。为了实现服务器和存储设备之间的通讯，通讯的两端都需要实现同样的通讯协议。存储设备上通常都有控制器，控制器实现了一种或几种通讯协议，它可以实现IDE,SCSI或光纤通道等存储协议到物理存储设备的操作协议之间的转换。而服务器的通讯协议是由扩展卡或主板上的集成电路实现的，它负责实现服务器内总线协议和IDE，SCSI等存储协议的转换。例如PC机中，一般主板上都有IDE协议的功能，IDE磁盘控制器上有IDE协议的功能。因此IDE磁盘可以连接到PC机的IDE连接线上。如果磁盘只支持SCSI协议，那么这种磁盘就不能直接与PC机连接。这时就需要在PC机扩展槽上插入一块SCSI卡，SCSI磁盘可以与卡连接。SCSI卡实现了PC总线到SCSI的转换。这种SCSI卡实现的功能就是主机总线适配卡的功能。如果磁盘只支持光纤通道协议，那么服务器上就需要支持光纤通道协议，因为光纤通道的高速特性一般服务器主板都不支持，需要专门的主机总线适配卡。服务器插入主机总线适配卡后，就可以与支持光纤通道的磁盘通过光纤通道连接了。

 



**故障转移：**

 

在计算机术语中，故障转移，即当活动的服务或应用意外终止时，快速启用冗余或备用的服务器、系统、硬件或者网络接替它们工作。 故障转移与交换转移操作基本相同，只是故障转移通常是自动完成的，没有警告提醒手动完成，而交换转移需要手动进行。



对于要求高可用和高稳定性的服务器、系统或者网络，系统设计者通常会设计故障转移功能。



在服务器级别，自动故障转移通常使用一个“心跳”线连接两台服务器。只要主服务器与备用服务器间脉冲或“心跳”没有中断，备用服务器就不会启用。为了热切换和防止服务中断，也可能会有第三台服务器运行备用组件待命。当检测到主服务器“心跳”报警后，备用服务器会接管服务。有些系统有发送故障转移通知的功能。



有些系统故意设计为不能进行完全自动故障转移，而是需要管理员介入。这种“人工确认的自动故障转移”配置，当管理员确认进行故障转移后，整个过程将自动完成。







**负载均衡：**

 

负载均衡（Load balancing）是一种计算机网络技术，用来在多个计算机(计算机集群)、网络连接、CPU、磁盘驱动器或其他资源中分配负载，以达到优化资源使用、最大化吞吐率、最小化响应时间、同时避免过载的目的。

使用带有负载均衡的多个服务组件，取代单一的组件，可以通过冗余提高可靠性。负载均衡服务通常是由专用软件和硬件来完成。







**Cluster：**

**
**



计算机集群（Cluster）简称集群是一种计算机系统， 它通过一组松散集成的计算机软件和/或硬件连接起来高度紧密地协作完成计算工作。在某种意义上，他们可以被看作是一台计算机。集群系统中的单个计算机通常称为节点，通常通过局域网连接，但也有其它的可能连接方式。集群计算机通常用来改进单个计算机的计算速度和/或可靠性。一般情况下集群计算机比单个计算机，比如工作站或超级计算机性能价格比要高得多。

 



**Switch:**

 

网络交换器（英语：Network switch）是一个扩大网络的器材，能为子网中提供更多的连接端口，以便连接更多的计算机。交换机工作于OSI参考模型的第二层，即数据链路层。交换机内部的CPU会在每个端口成功连接时，通过ARP协议学习它的MAC地址，保存成一张ARP表。在今后的通讯中，发往该MAC地址的数据包将仅送往其对应的端口，而不是所有的端口。因此，交换机可用于划分数据链路层广播，即冲突域；但它不能划分网络层广播，即广播域。

 



**软分区:**

 

软分区的含义是交换机将设备的全局名称放在一个分区中，而不管连接的是哪个端口。例如，如果全局名称Q和全局名称Z在同一个分区中，那么它们可以互相对话。相同的，如果Z和A又在另一个分区，那么Z和A可以看到对方，但是A不能看到Q.这是分区的复杂性部分；这种特点在以太网交换机中并不常见。



软分区的概念不难理解。它只是简单的表明架构是基于节点的全局名称。使用这种软分区的好处是，你可以连接到交换机的任何一个端口，而且如果你能看到其他节点，那么你也能访问这些节点。



从管理性的角度来看，软分区环境简直是一团糟。进行维护时，你必须知道每个节点连接到哪里。如果使用软分区，在交换机上就没有关于端口的描述，因为这些端口的信息很可能很快就过时。此外，软分区还有一定的安全风险。就每个人所相信的而言，没有人曾经看到过一个黑客正在试图哄骗全局名称的过程，但是这种行为是可能的。通过改变设备的全局名称来改变它的分区是非常困难的，因为黑客不知道哪些全局名称可以访问他所想要进入的分区。你总不会把自己的交换机设置信息放在大庭广众之下吧？

 



**硬分区:**

 

硬分区更类似以太网世界中的虚拟局域网。如果将一个端口放到一个分区，任何连接到这个端口的流量都是来自这个分区，或所设置的数个分区。当然，如果有人可以移动光缆的话，那么这种分区在面对物理攻击的时候就没那么安全了。但是，你需要担心这种情况吗？因此对于SAN来说，最好的设置是：交换机硬分区，并且对可以访问阵列端（target）逻辑单元号（LUN）的全局名称进行限制。你的存储阵列还需要全局名称屏蔽，以便多个发起端（initiator）可以被分区设置成可以同时看到阵列端。

一些人的分区架构想法很奇怪。将相同操作系统放在一个分区看起来是个好主义，但在实际上没有任何意义。过去人们总是很容易害怕将Windows服务器和使用不同操作系统的存储阵列放在同一个分区。当看到新的LUN时，Windows会弹出“你是否需要初始化新卷？”对话窗口，而且如果Windows管理员顺手决定点击“是”的话，那么他就破坏了其他人的逻辑单元号。如果存储阵列有逻辑单元号屏蔽的话，那么这就不成问题。

 



**SCSI：**

 

小型计算机系统接口（SCSI，Small Computer System Interface）是一种用于计算机及其周边设备之间（硬盘、软驱、光驱、打印机、扫描仪等）系统级接口的独立处理器标准。SCSI标准定义了命令、通信协议以及实体的电气特性（换成OSI的说法，就是占据了物理层、链接层、通信层、应用层），最大部份的应用是在存储设备上（例如硬盘、磁带机）；但，其实SCSI可以连接的设备包括有扫描仪、光学设备（像CD、DVD）、打印机……等等，SCSI命令中有条列出支持的设备SCSI周边设备。理论上，SCSI不可能连接所有的设备，所以有“1Fh - unknown or no device type”这个参数存在。

 



**Fibre Channel：**

 

光纤通道（Fibre Channel，简称FC）是一种高速网络互联技术（通常的运行速率有2Gbps、4Gbps、8Gbps和16Gbps），主要用于连接计算机存储设备。光纤通道由信息技术标准国际委员会（INCITS）的T11技术委员会标准化。INCITS受美国国家标准学会（ANSI）官方认可。过去，光纤通道大多用于超级计算机，但它也成为企业级存储SAN中的一种常见连接类型。尽管被称为光纤通道，但其信号也能在光纤之外的双绞线上运行。

光纤通道协议（Fibre Channel Protocol，FCP）是一种类似于TCP的传输协议，大多用于在光纤通道上传输SCSI命令。

 



**iSCSI:**

 

iSCSI又称为IP-SAN，是一种基于因特网及SCSI-3协议下的存储技术，由IETF提出，并于2003年2月11日成为正式的标准。与传统的SCSI技术比较起来，iSCSI技术有以下三个革命性的变化：



- 把原来只用于本机的SCSI协同透过TCP/IP网络传送，使连接距离可作无限的地域延伸；
- 连接的服务器数量无限（原来的SCSI-3的上限是15）；
- 由于是服务器架构，因此也可以实现在线扩容以至动态部署。

 



**FCoE:**

 

以太网路光纤通道标准（英语：Fibre Channel over Ethernet，缩写为FCoE），是一种通讯技术标准。它利用以太网路，传送光纤通道（Fibre Channel）的讯框，让光纤通信的资料可以在10 Gigabit以太网路网络骨干中传输，但仍然是使用光纤通道的协定。它属于INCITS T11 FC-BB-5标准的一部份。

 



**SATA:**

 

串行ATA（Serial ATA: Serial Advanced Technology Attachment）是串行SCSI（SAS: Serial Attached SCSI）的孪生兄弟。两者的排线兼容，SATA硬盘可接上SAS接口。它是一种计算机总线，主要功能是用作主板和大量存储设备（如硬盘及光盘驱动器）之间的数据传输之用。



2000年11月由“Serial ATA Working Group”团体所制定，SATA已经完全取代旧式PATA（Parallel ATA或旧称IDE）接口的旧式硬盘，因采用串行方式传输数据而得名。在数据传输上这一方面，SATA的速度比以往更加快捷，并支持热插拔，使计算机运作时可以插上或拔除硬件。另一方面，SATA总线使用了嵌入式时钟频率信号，具备了比以往更强的纠错能力，能对传输指令（不仅是数据）进行检查，如果发现错误会自动矫正，提高了数据传输的可靠性。不过，SATA和以往最明显的分别，是用上了较细的排线，有利机箱内部的空气流通，某程度上增加了整个平台的稳定性。



现时，SATA分别有SATA 1.5Gbit/s、SATA 3Gbit/s和SATA 6Gbit/s三种规格。未来将有更快速的SATA Express规格。

 



**RAID:**

 

独立硬盘冗余阵列（RAID, Redundant Array of Independent Disks），旧称廉价磁盘冗余阵列，简称硬盘阵列。其基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、容量巨大的硬盘。根据选择的版本不同，RAID比单颗硬盘有以下一个或多个方面的好处：增强数据集成度，增强容错功能，增加处理量或容量。另外，磁盘阵列对于计算机来说，看起来就像一个单独的硬盘或逻辑存储单元。分为RAID-0，RAID-1，RAID-1E，RAID-5，RAID-6，RAID-7，RAID-10，RAID-50，RAID-60。

简单来说，RAID把多个硬盘组合成为一个逻辑扇区，因此，操作系统只会把它当作一个硬盘。RAID常被用在服务器计算机上，并且常使用完全相同的硬盘作为组合。由于硬盘价格的不断下降与RAID功能更加有效地与主板集成，它也成为了玩家的一个选择，特别是需要大容量存储空间的工作，如：视频与音频制作。



最初的RAID分成了不同的等级，每种等级都有其理论上的优缺点，不同的等级在两个目标间取得平衡，分别是增加数据可靠性以及增加存储器（群）读写性能。这些年来，出现了对于RAID观念不同的应用。



















## 决定存储工作负载特征的几个要素和测量方法

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-08-03*

   存储的工作负载通常可以被分解成几个要素：随机的还是顺序的IO、大还是小的IO请求尺寸、读写请求的占比以及并发请求数。这些要素决定了你的工作负载是如何与存储组件交互的，并且最终决定了在给定的配置下，系统的运行性能。本文将介绍这些工作负载特征，帮助理解它们是如何影响到现实的存储系统的。



   注：本文的主要内容基于传统的机械硬盘（磁盘）。固态硬盘（SSD）相比传统硬盘取消了机械结构，其特征和最佳工作负载也有了很大不同，将在以后的文章阐述。

 

**随机的还是顺序的访问:**

 

   首先我们记住一个公式：IOPS = 1000 / (寻道时间+旋转延迟)



   寻道时间是指将读写磁头移动至正确的磁道上所需要的时间。从公式可以得出，寻道时间越短，I/O操作越快，IOPS就越高。



   旋转延迟是指盘片旋转将请求数据所在扇区移至读写磁头下方所需要的时间。旋转延迟取决于磁盘转速，通常使用磁盘旋转一周所需时间的1/2表示。比如，7200 RPM的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms，而转速为15000 RPM的磁盘其平均旋转延迟为2ms。



   注：事实上还有一个传输时间（同样位于公式的分母），即完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。由于主流的SAS、SATA接口数据传输率的不断提升，数据传输时间通常远小于前两部分消耗时间，简单计算时可忽略。



  想提高存储的IOPS性能，就要想办法减小分母上的数值。由于旋转延迟是固定的（除非购买更快转速的磁盘），所以唯一存在的变量就是寻道时间。很容易理解，顺序的工作负载越多，磁头需要（大幅度）移动的机会就越少，寻道时间也就越少。高度顺序性的工作负载最终的结果就是更快的磁盘响应时间和更高的数据吞吐量（throughput）。顺序的工作负载对于较慢的磁盘和RAID类型是一个不错的选择（如RAID 5、6）；而高度随机的负载更适合较快的磁盘和RAID类型（如RAID 10）。还有一种减少寻道时间的技术称为“磁盘短行程技术”（short stroking），即将数据存放在磁盘外缘的轨道上，从而减少寻道时间。类似的技术包括快道技术（fast track）、放射性数据部署技术（radial placement）等。



 顺序的工作负载类型有磁盘备份和SQL transaction日志文件写入等。顺序的负载类型包括OLTP数据库访问和Exchange Information Store的读取访问。但多数大型的应用都是混合了顺序与随机的读写负载，比如VMware vSphere虚拟化环境。

 



**IO请求尺寸：**

 

   IO请求大小也是工作负载的一个重要因素。一般而言，大的读写IO请求比小的IO在一定程度上更有效率。更大的IO请求（比如64KB相比2KB）可以得到更高的数据吞吐量和更少的处理时间。 大多数工作负载限制了IO大小，但是了解应用的IO尺寸可以帮助你决定合适的存储系统参数，如阵列的条带大小（stripe size）和文件系统簇大小（cluster size）。关于这方面的设定，建议参考应用厂商和存储厂商的推荐配置文档。



   如果是Windows服务器，可以使用性能检测工具perfmon中的Avg. Disk Bytes/Read计数器来查看平均的 IO尺寸。如果是VMware虚拟化工具，可以使用官方的vscsiStats工具（见下文）。

 



**读写占比：**

 

   每一种工作负载都由不同数量的读写操作组成。有时一些特定的负载，比如Exchange可以被分解成几个子负载：写密集型的日志记录和读密集型的数据库访问。了解负载的读写占比可以帮助你设计底层的存储系统。由于RAID写惩罚的存在，一个写密集型应用在RAID 10的LUN比在RAID 5上要表现得更好。这方面的内容可以阅读论坛之前的文章：

   [如何计算IOPS？](http://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651771060&idx=2&sn=86ba15b5d1655e95388bcb952745d99b&scene=21#wechat_redirect)

   [浅谈RAID写惩罚（Write Penalty）与IOPS计算](http://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651770875&idx=2&sn=3cacff37ee702a37d6140bad89afde5b&scene=21#wechat_redirect)

 



**并发的和待处理IO：**

 

   一些工作负载支持多线程的IO请求。这些负载会对存储系统造成极大的压力（无论是IOPS还是吞吐量），所以在设计时必须注意。多路径（multipathing）软件可以提升多线程环境下的效能。一个典型的VMware vSphere环境就是支持多线程待处理IO请求队列的实例。

 



**如何测量工作负载特征：**

 

   那么如何测量应用和存储的工作负载特征呢？首先可以寻求厂商的帮助，包括应用、操作系统和存储设备厂商。操作系统自带的一些工具比如Windows Performance Monitor可以获取一些参数（[使用Windows Perfmon看存储性能问题](http://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651771261&idx=1&sn=39834f99b35a3244ee18adda14941d40&scene=21#wechat_redirect)）。存储端的软件比如EMC的Unisphere Analyzer通常可以拿到更为详尽的数据（[Navisphere/Unisphere Analyzer性能分析工具](http://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651771039&idx=3&sn=029c78922a202363696ba013a5286c4c&scene=21#wechat_redirect)）。还有很多免费的主机端存储性能测试软件，如IOZone、Iometer、FIO等，它们的功能比较可以看一下之前的讨论：[常用免费存储性能测试工具的简单比较](http://mp.weixin.qq.com/s?__biz=MjM5NjY0NzAwMg==&mid=2651771261&idx=2&sn=77c8abf8c339d665a6d2c894bc6247ac&scene=21#wechat_redirect)。



   VMware的vscsiStats更是一个有针对性的工具。由于所有的IO命令都会经过Virtual Machine Monitor (VMM)，Hypervisor可以轻松获得VMware中虚拟机的工作负载特征，这些参数包括：ioLength、seekDistance、outstandingIOs、latency、interarrival等。vscsiStats从ESXi 4.1起就自带了。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUTCWxtvYs5cwhM4ic3niabqOlY6LAjFvjl4MplHgHhLkCeoTfqjzWRGtsh50UbZEOU53wDx9wnbQJQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUTCWxtvYs5cwhM4ic3niabqO0CxDXZq4te7wAmE9vURibeWE8taHOl7VDyfdbR6Wa9B6nviciaZibmribEw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 

 

   **vscsiStats相关的使用介绍：**

   Using vscsiStats for Storage Performance Analysis

   vscsiStats: Fast and Easy Disk Workload Characterization on VMware ESX Server

   Storage Workload Characterization and Consolidation in Virtualized Enviornments

   另外VMware的esxtop工具也能用于存储工作负载和存储性能分析，但它暂时只支持Fibre Channel和iSCSI协议。参考：Using esxtop to identify storage performance issues for ESX / ESXi。



   需要注意的是，使用vscsiStats和esxtop都需要进入到ESXi Shell，即以前的Tech Support Mode模式。方法可以参考VMware文档：Using ESXi Shell in ESXi 5.x。

















## 什么是Hadoop即服务（Hadoop-as-a-Service）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-07-31*

  对于了解大数据的人肯定不会对Hadoop陌生，那到底什么是Hadoop即服务（HaaS）呢？本文将带你来一探究竟。

 

**HaaS出现背景:**

 

  开源大数据框架Apache Hadoop已经成了大数据处理的事实标准，同时也几乎成了大数据的代名词，虽然这多少有些以偏概全。根据Gartner的估计，目前的Hadoop生态系统市场规模在7700万美元左右，2016年，该市场规模将快速增长至8.13亿美元。



  但是在Hadoop这个快速扩增的蓝海中游泳并非易事，不仅开发大数据基础设施技术产品这件事很难，销售起来也很难，具体到大数据基础设施工具如Hadoop、NoSQL数据库和流处理系统则更是难上加难。客户需要大量培训和教育，付费用户需要大量支持和及时跟进的产品开发工作。而跟企业级客户打交道往往并非创业公司团队的强项。此外，大数据基础设施技术创业通常对风险投资规模也有较高要求。



   种种这些就催生了众多Hadoop作为一种服务（HaaS）提供商的诞生。HaaS为不堪重负，渴求Hadoop，但又缺乏相应的内部资源或专业知识的数据中心管理员们提供了一个绝佳的机会。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXxb7ic8ZsGw6cCKWcsiaLLdQHBVKx5QKjcGmnbrd5zefCOWYtcDophyEpMSkZNqmLMaIy7bLLTV4NQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**HaaS的价值**

 

  与直接在物理机上部署Hadoop相比，很明显HaaS可以做到按需购买、按需使用，并且只为使用时间付费。同时，和其他“…即服务”的模式一样，如果你不再需要Hadoop环境了，现有的资源可以被用于其他的工作负载。



  在物理机上部署Hadoop通常还需要专项的资金投资、数据中心的机柜空间、精密空调、电力和其他各种技术问题。而对于HaaS，用户需要考虑的只是管理一些额外的虚拟设备，或者投入一些资金在设备群集上。

 

**HaaS标准：**

 

  用户需要什么样的HaaS呢？每家服务提供商之间的差别是巨大的。HaaS供应商们提供一系列的功能和支持，从基本的访问Hadoop软件到虚拟机，从“自行运行”（RIY）环境软件的预配置到包括工作监督和调整支持的全方位服务支持。对于HaaS的任何评价都应该考虑到如何更好的让每一项服务能够满足您的业务目标，同时尽量减少Hadoop和基础设施的管理问题。下面我们列举五个标准，也许可以帮助您区分不同的HaaS备选方案。



- **HaaS应同时满足数据科学家和数据中心管理员的需求**

  数据科学家们花费了大量时间进行处理数据，整合数据集及应用统计分析。这些类型的数据用户通常会希望有一个功能丰富且强大的环境。理想情况下，数据科学家们应该具备通过Hive、Pig、R、Mahout及其他数据科学工具运行Hadoop YARN作业的能力。数据科学家一登录到服务，相关的计算操作就应立即可用，并开始工作。集群启动和重新加载数据的延迟是低效和不必要的。“永远在线”的Hadoop服务，避免了数据科学家必须在开始工作之前从非HDFS的数据存储集群和负载数据部署出现的令人沮丧的延迟。而对于系统管理员，少即是多。他们的工作就是进行一系列的相关管理工作。管理控制台应简化，使他们能够迅速的通过执行数量最少的步骤就能完成这些任务。如果管理员必须配置一组参数，那么就应该同时避免这些参数被暴露，又要避免参数被HaaS 供应商管理。同样的，低层次的监控细节应由HaaS 供应商管理。管理界面应该能够简单明了的反应管理平台的整体状况和是否遵从了SLA。

- **HaaS应该在HDFS存储“静态数据”**

  HDFS是存储在Hadoop的数据的原始格式。当数据需要被持久的以其他格式存储时，其必须被加载到HDFS中。持久地在HDFS中存储数据，避免了延误，以及将数据从另一种格式转换到HDFS的成本。

- **HaaS应该提供弹性**

  当企业用户在考虑是否选择某家HaaS供应商，并进行相关的评估时，弹性应成为考虑的中心要素。而在考虑是否选择某家HaaS供应商时，需要考虑进行评估的另一个因素是HaaS供应商根据服务管理需求提供弹性的难易程度。特别是，必须考虑到服务处理不断变化的计算和存储资源需求的透明度。

- **HaaS应支持不停机操作**

  在有固定工作负载的生产环境中，系统管理员可以调整操作系统和应用程序来优化这些工作负载的处理。他们可以通过各具特色的配置参数的最佳设定和监控操作的关键指标，以确保工作按预期运行，实现不停机操作。

- **HaaS应该是自配置**

  使用HaaS的优点是，其最大限度地降低对Hadoop专家的需求。HaaS本身能够自行配置最佳数量和类型的节点。数据科学家们深谙统计和机器学习何时可能需要应用特定的统计测试或使用特定的机器学习算法，但对于一个Hadoop集群的配置来保持他们的工作流程的运行则可能并没有很深的造诣。



















## 初级DBA需要知道的十件事

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-08-01*

   成为一个合格的DBA（Database Administrator，数据库管理员），光了解SQL语句是不够的。DBA通常工作在数据库、服务器、运维人员和开发者的交集上，所以一个合格的DBA需要了解IT的方方面面，从而可以具备对性能、硬件和软件问题排错的能力。本文将介绍一个初级的DBA需要知道的十件事，快来看看自己是否已经都具备这些能力啦？



**备份和恢复:**

 

   任何一个名副其实的DBA都应该了解如何使用DBMS（数据库管理系统）的内建工具来备份和恢复数据，比如Oracle Recovery Manager和SQL Server Management Studio。除此之外，了解有哪些第三方的数据库备份工具也是很有必要的。了解过后你会发现，这些工具各有优势。事实上，仅仅备份了数据库文件并不代表这个备份是一个“好”备份。

 

**基本的优化：**

 

   当创建索引时，DBA需要知道如何提出优化建议。你需要知道一些基本的索引策略，同时还要知道下面这些问题的答案。比如，何时引入聚集索引（Clustered Index）？何时使用覆盖索引（Covered Index）？数据库优化器如何工作？它是否依赖于特殊表的统计数据？如何更新这些数据？使用优化器对重组表和索引意味着什么？它们应该如何被重组，以及是否能自动化这一过程？

 

**协助软件开发者：**

 

   软件开发人员可以搭建也可以摧毁你的数据库。帮助他们撰写有效率的查询代码是很重要的。你要帮助他们了解，一次发起1个查询请求比同时发起1000个请求要有效率得多：大数据情况下，1次查询返回1000行结果比1000次查询每次返回一条快得多。作为一个DBA，帮助他们理解有些时候在DBMS执行操作要比在代码中更好。典型的案例就是，通过网络抓取大量数据再在本地合并，很有可能比直接一个合计函数（Aggregate Function）查询慢。

 

**存储系统：**

 

   大多数数据库的性能瓶颈在磁盘，了解你的数据库位置所在以及DBMS是如何访问物理数据也是非常重要的。如果你的企业有存储团队，联系他们，并且掌握他们是如何监控存储的一些重要性能指标，如IOPS和响应时间。

 

**了解查询计划：**

 

   一个初级DBA应该知道如何生成和阅读基本的查询计划。并不一定要求你完全理解所有的内容，但几个关键的过程还是要掌握的，比如随意的全表扫描（Full Table Scan）和嵌套循环（Nested Loops）可能会出现问题。同样，你需要知道何时优化器会推荐更改，为什么这样的更改会生效，以及这些操作会对系统造成怎样的性能妥协。

 

**了解规范化：**

 

   规范化的数据库表（Normalized Tables）是一个优良设计的关系型数据库的基础，但有时这也会带来灾难。DBA需要理解并知道如何将数据纳入第一、第二和第三范式。为什么规范化很重要以及何时它可能成为一个不利因素？知道主键、外键和唯一键的区别，同时知道如何强制一对一、一对多的关系。

 

**了解SQL语言：**

 

   DBA还需要掌握SQL DML（数据操作语言）和DDL（数据定义语言）。DML包含的项目包括Select（查询）、Update（更新）、Insert（插入）和Delete（删除）。DDL包含Create Table（创建表）和Alter Table（删除表）。初级DBA应该了解如何创建和修改表以及索引，并且知道删除记录、截断表和丢弃表间的区别。当然，也不能忘了视图（View）。

 

**操作系统：**

 

   DBA还需要熟悉操作系统，知道不同操作系统之间的差别，比如安全设置、与活动目录（AD）、LDAP的集成和命名规范。同时你还需知道数据库是如何启动的，需要哪些脚本来启动、关闭或临时锁住用户的访问。

 

**脚本：**

 

   为了让工作更有效率，脚本是必不可少的。想象一下你有十个或更多的数据库需要管理，你是否只能逐一登录并手动启动/关停它们？快去找本脚本指南来看看如何批量地执行这些操作吧。

 

**存储过程和触发器：**

 

   将存储过程和触发器单独列开一项是因为考虑到这更像是编程而不是“组合”SQL语句。不管怎样，你需要知道何时使用它们，并让开发团队知道不在代码中完成操作的优点。同样，许多第三方应用会自带需要的存储过程和触发器。能够读懂这些过程并了解它们的逻辑，对将来性能问题的排错很有帮助。能越快理解这些过程，你就能对自己管理的数据库更得心应手。



















## 什么是固态阵列SSA（Solid-State Array）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-06-19*

   根据知名咨询公司Gartner发布的《市场份额分析报告：2013年全球SSD与固态硬盘阵列》，此前普遍采用的全闪存阵列（All-Flash Array，AFA）称谓被调整为固态阵列Solid-State Array (SSA)。那到底相对于AFA，SSA有哪些不同？本文来带你一探究竟。

 

**固态阵列:**

 

   Gartner对固态阵列SSA的定义非常严谨：



1. SSA属于具备可扩展性的专有解决方案，完全基于固态半导体技术以实现数据存储功能，而且任何时候都无法利用HDD技术进行配置。不同于ECB（即基于控制器的外部存储）阵列中的纯SSD盘架；
2. SSA必须是具备特定名称与型号的独立产品，其通常（但并不总是）包括针对固态存储技术的操作系统与数据管理软件。

   

​    正因如此，部分友商，包括EMC自己的VMAX和VNX都被排除在外。这也是为什么EMC之前就布局并推出了全新的全闪存版VNX即VNX-F系列。除了VNX-F，EMC入选的产品还有两年前收购的XtremIO。

 



**全闪存发展三阶段：**

 

   以EMC收购的全闪存阵列（即现在的固态阵列）产品XtremIO为例，我们可以将固态阵列的发展分为了三个阶段。其中阶段一从2008年开始，是以追逐性能为主的应用程序加速应用，很简单，看重SSA的高IOPS的能力，这也被称为“高性能”市场。处在这个阶段的厂商非常多，竞争处于白热化状态。



   SSA成长的第二个阶段从2013年开始，强调应用程序和数据服务，听起来不是很好理解。但只有这个市场的潜能被激发，SSA市场的占比就会达到50%左右。



   SSA成长的第三个阶段目的是实现数据中心的敏捷性，包括实现池化的存储、实时分析、实时开发等。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUTCWxtvYs5cwhM4ic3niabqOFUCtaHHMXtWrRMQYeIMMV5drVNYWmTHhHPaictAq80GdDuQElUBmGIw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

















## 什么是超融合系统、集成系统和参考架构

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-03-16*

   EMC推出的超融合架构产品EMC VSPEX BLUE，将计算、存储、网络、管理集于一身，基于VMware EVO: RAIL架构理念和EMC软件打造。VSPEX BLUE的横向扩展基础架构能够帮助客户以同步于业务变化的速度，满足用户不断变化的需求，并摆脱进行复杂IT预测的负担。



   那到底什么是超融合架构系统（Hyper Converged System），它与以往传统的集成系统（Integrated System）和参考架构（Reference Architecture）有什么区别？本文来带您一探究竟。

 

   IT架构的市场正走在快速更新与合并的道路上。由业务敏捷性驱动的客户需求已经从单一的计算能力、网络和存储架构往云计算、应用现代化和工作负载管理方向迁移。这一趋势造成的结果就是，厂商开始关注与产品创新并扩展相应的产品线。这些创新其中之一就是聚合计算能力、存储服务并让它们在同一套物理设备上运行。渐渐地，这些计算能力、存储和网络软件松开了原先与底层架构紧耦合的联系，取而代之地以各种软件定义的形式运行在了标准的x86服务器上。于是更多的工作负载运行在了虚拟架构上。



   如今，超融合架构原生地将核心计算力、存储和网络功能整合进了一个单一的软件方案或设备中。新的超融合系统包含一个运行在一个或多个节点上的分布式软件栈。每一个节点包含一个分布式文件系统或对象存储库，更有一个虚拟主程序（Hypervisor）栈引导硬件设备并将其抽象成更多的CPU、内存和硬盘资源。超融合群集中的节点通过内建的网络（以太网或者InfiniBand交换机）通讯，也可以经由用户提供的后端网络来通讯。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXFDsafwOibSCSqbuxAe5Vmmt2gX1kBW7cszUAGtebricvPqBhfQlIFkFNdExFxubIp5B1LR7gialICg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



   超融合系统、集成系统和参考架构在架构上的不同如下：

​           

|                  | 超融合系统（除软件外）                                       | 集成架构系统                                                 | 参考架构（经认证的多厂商）                                   |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 概览             | 单库存单位（SKU）的产品或设备，完整的开发生命周期（如VSPEX  BLUE或其他EVO: RAIL设备） | 预集成、经厂商认证的解决方案，以一整套系统发售（如VCE Vblock） | 方案基于来自多个技术厂商的自治的系统；使用高度规范的框架和配置模板 |
| IT组件           | 使用原生的、基于服务器的存储、计算与网络功能；存储和计算功能运行在单一节点或设备群集之上 | 使用独立自治的、具有单项优势的服务器、存储和网络组件，所有设备通过外部网络相连 | 使用独立自治的、具有单项优势的服务器、存储和网络组件，所有设备通过外部网络相连 |
| 设备集成前的责任 | 无——超融合系统以整套设备（Appliance）的方式交付，不需要预先整合 | 由IT厂商负责预集成                                           | 合作伙伴——经认证的合作伙伴负责多数或全部的预集成和部署       |
| 新知识产权       | 专有的超融合软件栈运行在每个节点上，负责计算、数据组织、完整性、集装化和硬件抽象服务 | 不同组件和系统的组织和整合                                   | 无——配置在已有的设备上定义和配置                             |
| 服务和支持模式   | 通常由单一的IT厂商提供支持                                   | 通常由单一的IT厂商提供支持                                   | 由厂商和渠道合作伙伴以高度协作的方式提供支持                 |

















## 闪存存储常见问题及术语定义

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-07-29*

   闪存存储是指将闪存用作存储介质。闪存有多种使用形式，主要用于主内存、内存卡、USB 闪存驱动器和固态驱动器。相比传统旋转式硬盘驱动器，闪存存储可大幅提升性能。虽然它们的每GB成本较高，但可以通过重复数据消除和压缩等数据减少技术削弱这一缺陷。



   换个角度来看，闪存存储大幅降低了单位操作成本，即每IO成本。这可以为大多数业务工作负载以及虚拟数据中心部署节省大量资金成本（后者在现代数据中心几乎已普遍存在）。虚拟化导致单位服务器的 IO 密度大幅上升；通过利用闪存存储技术，可显著影响此工作负载模式，使其性能提高、成本降低。



本文将介绍闪存存储一些常见问题的解答以及相关术语定义。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIVk6QbDp9W2HcvPkOBKwVv1WdUwYhNP2gTMAuQQmbsxicb47Klib8tB2NbAkRrs8icoHeOR27oGOxEVQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**常见问题解答:**



1. **什么是闪存存储？它有什么用？**

   闪存存储指的是任何使用闪存的存储库。闪存有很多不同的外形尺寸，您可能每天都在使用闪存存储。闪存存储可以说无处不在，从通过USB连接到您的计算设备上的简单电路板上的一块闪存芯片，到您的电话或MP3 播放器中的电路板，再到完全集成的“企业级闪存磁盘”－ 连接许多芯片并且具有特定外形尺寸的电路板，用以代替旋转磁盘。闪存存储无处不在！

2. **什么是闪存存储SSD？**

   固态磁盘或企业级闪存磁盘(EFD)是一种完全集成的电路板，上面集成了许多闪存芯片来代表一个闪存磁盘。SSD 主要用于替代传统的旋转磁盘，它在MP3播放器、笔记本电脑、服务器和企业存储系统中都有使用。

3. **闪存存储和SSD的区别是什么？**

   闪存存储指的是任何可以用作存储库的设备。闪存存储可以是简单的 USB 设备，也可以是完全集成的全闪存存储阵列。固态磁盘(SSD)指的是用于代替旋转介质的集成设备，常用于企业存储阵列。

4. **闪存存储和传统硬盘有什么区别？**

   传统硬盘利用旋转盘片和旋转磁头从磁性设备读取数据，类似于传统的留声机；闪存存储利用电子介质或闪存，以大幅提升性能。闪存消除了旋转延迟和寻道时间，而正是它们造成了传统存储介质的延迟。

5. **全闪存阵列和混合阵列有什么区别？**

   混合存储阵列使用旋转磁盘驱动器和闪存SSD的组合。在配备恰当软件的情况下，混合阵列经配置后可以提高总体性能并降低成本。全闪存阵列只支持SSD 介质。

 

**术语定义：**

 

  **闪存（Flash Memory）**

  闪存是一种非易失性读/写半导体存储器，用于固态存储设备。闪存在单元中存储数据位。最初，闪存被设计为每个单元存储一位，这称为单层单元(SLC)闪存。之后的各代闪存产品都被设计为每个单元存储两位或多位。这称为多层单元(MLC)闪存。当然，与SLC闪存相比，MLC闪存是密度更高的内存，因而MLC闪存每位的成本也就比 SLC 更低。不过，MLC闪存的耐久性低于SLC闪存。



  **垃圾数据收集（Garbage Collection）**

  垃圾数据收集在后台运行，它累积之前标记为删除的数据块，对各个“垃圾”数据块执行完整的数据块擦除，然后返还回收的空间，以供后面的数据块重新使用。



  **SSA（Solid-State Array，固态阵列）**

  SSA是范围更广的ECB存储市场的一个新的子类别。SSA是可扩展的专用解决方案，主要以固态半导体技术为基础，用于不能随时使用HDD技术配置的数据存储。由于和ECB存储阵列中只支持SSD的机架截然不同，SSA必须是以特定的名称和型号标示的独立产品，该产品通常（但不总是）包括操作系统和针对固态技术优化的数据管理软件。



  **SSD（Solid State Drive，固态驱动器）**

  可以利用传统 HDD外形尺寸（如3.5 英寸、2.5 英寸或 1.8 英寸）的固态存储。固态驱动器通常使用存储接口，如SATA、SAS或光纤通道。



  **SSS（Solid State Storage，固态存储）**

  任何由非移动存储器技术（而不是移动的磁性或光学介质）提供的存储能力。固态存储通常具有非易失性的特点，并且具有多种外形，例如固态驱动器、固态卡或固态模块。使用的典型接口包括SATA、SAS、光纤通道或PCIe。



  **吞吐量（Throughput）**

  在指定的时间段内，能够从设备传送过来（读取）或者是传送到设备（写入）的数据量，通常使用兆字节/秒(MB/s)作为单位。吞吐量体现了设备在应用程序生成顺序读/写操作时的性能。



  **磨损均衡（Wear Leveling）**

  闪存控制器使用的一组算法，用来跨闪存单元分配写入和擦除操作。磨损均衡的目的是，推迟单个单元的磨损，并延长闪存存储设备的使用寿命。



  **写入放大率（Write Amplification）**

  由于之前写入的NAND闪存位置必须在擦除之后才能重写，因此闪存固态设备中的写入操作数通常都大于主机发出的写入数。“写入放大率”可以用等式来表示：写入放大率=（写入到闪存的数据）/（主机写入的数据）



















## 融合基础架构概念解析

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-07-28*

   融合基础架构是诸多巨头和行业联盟都在提的一个理念，通过将计算、存储和网络融合到一个架构来降低数据中心的管理难度。在交付给用户之前，融合基础架构使用综合的管理工具来提供对整个架构的统一管理，并且融合基础架构中的所有组件之间都经过了预调试，保证能够协同工作。将计算和存储资源融合于单一设备，可提高数据中心的整体灵活性，并尽量降低延迟时间，本文将总结融合基础架构的概念和优势。 

 

**融合基础架构定义**

 

融合基础架构是一种通过单一控制面板进行数据管理的方法，可有效实现 IT 资源的集中化。融合基础架构的组件可能包括服务器、数据存储设备、网络设备以及用于IT 基础架构管理、自动化和流程编排的软件，然后融合基础架构以“打包”的方式交付给用户。IT 组织可以利用融合基础架构来整合系统、提高资源利用率并降低运营成本。借助融合基础架构，您能够实施可由多个应用程序共享并集中管理的计算、存储和网络资源池，从而有助于上述目标的实现。

 

融合基础架构可以满足你全新构建的需求，企业用户可以选择来自一家厂商的融合基础架构产品，也可以集成多家厂商的设备。并且大多数融合基础架构支持几乎所有的hypervisor，企业用户可以按需进行灵活部署。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIVk6QbDp9W2HcvPkOBKwVv1jGkvv7sGbHcRNOFPaAKtUGfYnJkehvWcGDN9Mp4Hic1qaRtibWMRTYXw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



 

聚合基础架构管理统一了故障、性能、流量、容量、应用响应管理等，从而迅速将大量不同数据转化为可操作洞察力，进而提高服务质量、可预测性和拥有成本。

 

**• 将所需的一切集中在一处**

技术领域内的统一管理以其开放的集成体系结构和广泛的第三方平台支持使IT人员可以将更多的时间用在主动管理环境上，而不是管理从不同工具提取和收集数据的过程。技术领域内的受监控数据的单个性能视图使IT人员不再需要搜寻多个工具来查找用于分类性能问题和确定根本原因的相关信息。

 

**• 用于快速分类和解决问题的可操作智能**

从技术领域和多种监控功能收集大量原始数据，标准化这些数据以用于分析，组织这些数据以根据用户角色和要求提供最相关的信息。在单个基于Web的用户界面中显示数据和分析，其中向导式工作流根据聚合基础架构和应用的最佳实践来有效指导IT运营和工程团队完成故障排除过程。

 

**• 适用于最大环境的大规模可扩展性**

先进的体系结构可提供无代理数据收集、一流的数据汇总技术以及动态联合分析，从而快速将数百万指标转化为可操作智能。该体系结构呈线性扩展，以管理最复杂的IT基础架构的增长。小型主机设备和集中式管理功能可降低资本和运营支出。

 

 **• 多租户访问控制**

本机多租户可确保控制用户对租户数据的访问。使托管服务提供商和企业IT人员能够为每个用户组/客户、部门、位置等分离监控环境。通过单个用户界面管理所有租户，从而降低管理开销。





















## 超融合基础架构概念解析

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-03-24*

由服务器搭配独立存储系统构成的传统IT基础设施，无论在技术架构还是在成本方面，都已经无法承载持续暴涨的数据处理需求，不仅扩展能力有限，随着应用规模的扩大，成本也将逐渐攀升到用户难以承担的程度。超融合系统正是在这样的背景下诞生的，通过借鉴Google、Amazon、Facebook这些超大型服务商应对大量信息处理需求的经验，超融合系统通过分布式系统软件，将大量服务器组成大规模的集群。

 

**超融合基础架构定义**

 

  超融合基础架构是一种主要由软件定义的系统，系统中不仅仅具备计算、网络、存储和服务器虚拟化等资源和技术，而且还包括缓存加速、重复数据删除、在线数据压缩、备份软件、快照技术等元素，而多节点可以通过网络聚合起来，实现模块化的无缝横向扩展（scale-out），形成统一的资源池。它不同于传统的融合基础架构；在传统的融合基础架构中，以上几种资源通常都分别由起到单一作用的分离组件来处理。超融合基础架构还提供了具有高效可扩展性的虚拟化就绪环境。此外，由于简化了采购和部署并降低了管理成本和复杂性，它还可能实现资本和运营支出的减少。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXWqdO54moHSczxEuyB3aecPyeK6uvYbF6QwlgOzDZ4icmZn5tQoTsM5s4PCEh0oxEaWicoxzWzoDNQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**超融合基础架构优势**

 

  与集成基础设施系统和平台细分市场相似的是，超融合基础设施细分市场也集成了一系列技术，跨越存储、计算、网络、基于管理程序虚拟化，容器系统及基础设施管理等功能区域。通常人们认为超融合基础架构具有下列优势：

 

- 以软件为核心，软件定义数据中心
- 通用的x86节点融合了存储、计算、网络、虚拟化平台（hypervisor）
- 分布式存储架构，通过增加节点的方式横向扩容
- 高度自动化，部署、维护简便
- 众多节点组成一个整体，统一管理分配资源

 

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXWqdO54moHSczxEuyB3aecH9DgBKicvpK38y6CKlgoleiciarIYk5ILP3sicico00TjzQPydvib6Wab2wg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



















