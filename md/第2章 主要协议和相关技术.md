# **第2章 主要协议和相关技术**









## 【存储入门必读】SCSI访问控制原理介绍

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-03-10*

本文为大家介绍SCSI-2和SCSI-3访问控制原理。主要内容包括：SCSI-2 Reserve/Release/Reset和SCSI-3 Persistent Reserve IN/ Persistent Reserve OUT/ PREEMPT以及SCSI访问控制常见场景。

 

------

**
**

**SCSI-2 Reserve(预留)/Release(释放)/Reset（重置）**

 

**SCSI-2协议中客户端访问lun过程如下：**

1. 客户端向lun发起预留操作
2. 预留操作成功后，客户端获得lun操作权限；预留失败，提示预留冲突，会继续尝试，直到预留成功。
3. 客户端操作完毕后，执行释放操作，其他客户端可以预留。



**SCSI-2访问控制主要缺点有：**

1. 预留操作基于路径。预留和释放必须由相同的客户端完成，一台主机不能释放另外一台主机的预留，同一主机HBA卡不能取消相同主机另外一块HBA的预留。
2. 预留无法长久保留。主机重启将会丢失预留信息。
3. 如果lun已经被预留，其他主机无法再预留。如果其他主机要想获得lun操作权限，必须对lun进行重置，重置操作可能会导致数据丢失。重置后释放掉lun现有的预留，重置操作由lun主动发起，原来预留主机并不知晓。

 

------



**SCSI-3 Persistent Reserve (PR)/ PREEMPT（抢占）**

 

SCSI-3协议引入PGR（persistent group reservation）功能。在访问lun之前，客户端首先向lun注册（registration）一个预留密钥(reservation key)，注册成功后客户端可以尝试进行永久预留（reserve），永久预留成功后就可以获得lun操作权限。预留密钥是一串16进制的ASCII码，最长8个字节。永久预留一共6种类型，由1、3、5、6、7、8数字表示。包括两种操作类型和三种客户类型，操作类型包括写排它和所有访问排他，客户类型包括所有客户端、已注册客户端和所属客户端。数字与永久预留类型对应关系如下：

1-> write exclusive

3-> exclusive access

5-> write exclusive - registrants only

6-> exclusive access - registrants only

7-> write exclusive - all registrants

8-> exclusive access - all registrants.



不同注册类型对应不同访问权限。与SCSI-2不同，SCSI-3释放操作根据预留密钥。不同客户端可以使用相同密钥或是不同密钥进行预留，具体可以结合永久预留类型决定。客户端可以通过抢占来获取已被永久预留的lun访问权限。SCSI-3抢占和SCSI-2重置不一样，抢占不会造成数据丢失。

SCSI-3关于PGR相关操作命令分为两大类：分别是PRIN和PROUT。PRIN主要用于查询，PROUT用于修改。SCSI命令执行过程中，需要明确该命令是哪种类型。

 

------



**常见使用场景**



1. **集群I/O Fencing**

   为了防止集群故障发生“脑裂”现象，2-节点集群可以通过SCSI-2 Reseve/Release触发I/O fencing来保证整个集群正常运行，是SCSI-2不适用于多-节点集群，多-节点集群可以使用SCSI-3 PGR。主流厂商集群套件都已经支持SCSI-3 PGR，比如：VCS、HACAMP、RHCS等。

   

2. **集群文件系统**

   集群文件系统需要保证多节点同时访问存储时的数据一致性，SCSI-2/SCSI-3都可以满足，当一个节点尝试访问一个已经被预留的存储就会产生访问权限冲突。SCSI-3 PGR相比SCSI-2 Reserve/Release更能够减少访问权限冲突。

 

------



**小结：**

 

SCSI-2具体基本访问控制能力，但是无法满足Active/Active多路径环境和集群多节点访问存储的需求。SCSI-3通过引入客户端注册和操作权限分类概念，强化并行访问权限控制，弥补SCSI-2的不足。

















## Cisco MDS 9000系列FCIP配置文档 – FCIP基本概念

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-03-22*

**什么是FCIP？**



Fibre Channel over IP protocol（FCIP）是一种隧道协议。将多个物理独立分步的光纤SAN环境，通过IP LAN/MAN/WAN连接起来。示意图如下：

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXWqdO54moHSczxEuyB3aecQXlvJ70t6EZ4Aot1XRRSZT6up5uZhK7Xc3YIUk45ceQYAJ1AMAmL7w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

FCIP使用IP层作为网络，TCP作为传输层，TCP头部的DF位设置为1.

备注：更多关于FCIP协议信息，请参考IETF标准关于IP存储章节http://www.ietf.org；或者参考光纤协议标准关于switch backbone connection章节http://www.t11.org（FC-BB-2）.





本系列包含以下主要章节：

- FCIP概念
- FCIP高可用方案
- FCIP参数配置
- FCIP配置过程
- FCIP其他功能特性



**FCIP基本概念：**

 

FCIP功能支持可以通过Cisco IPS（ip storage）模块或者MPS (multiprotocol service) 模块获得，FCIP基本概念如下。



**FCIP和VE_Port**



下图结合Fibre Channel ISL和Cisco EISL描述FCIP内部模型。

FCIP virtual E(VE) Ports在逻辑上于标准Fbire Channel E Ports一样，只是使用FCIP协议封装而不是Fibre Channel。FCIP协议要求链路两端都是VE Ports。



虚拟ISL链路通过FCIP链路建立，并在之上传输Fibre Channel数据帧。虚拟ISL链路和Fibre Channel ISL一样，两端是E Port或者TE Port。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXWqdO54moHSczxEuyB3aecslZGwmd0IEVPniaoHgSHNQr9liaLcBkHpO2LDL3ibSo4S1rEiaao4BddQA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**FCIP链路**



FCIP链路由两个FCIP终端之间的一个或者多个TCP连接组成。每个链路携带FCIP协议封装过的光纤帧。当FCIP链路启动，FCIP链路两端的VE端口会创建一个virtual (E)ISL链路，并且初始化E端口协议拉起（E）ISL链路。



默认情况下，Cisco MDS 9000系列交换机会为每个FCIP链路创建两个TCP连接。

1. 一个连接用于数据帧传输
2. 另外一个连接用户Fibre Channel控制帧（所有F类型帧）。专门一个连接用于传输Fibre Channel控制帧是为了保证控制帧低延迟。

 

在IPS或者MPS模块上使用FCIP功能之前，首先需要配置FCIP interface和FCIP profile。



FCIP链路再两个节点之间建立成功只是，VE Port初始化过程与E Port一样。初始化过程与FCIP或者Fibre Channel无关，而是基于E Port发现过程（ELP，ESC）。在Fibre Channel层，E Port和VE Port是一样的。



**FCIP Profile**



FCIP profile包含本地IP地址和TCP端口灯参数配置信息。FCIP profile的本地IP地址具体FCIP链路使用哪个Gigabit以太网口。



FCIP Profile与FCIP链路关系图如下：

 

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXWqdO54moHSczxEuyB3aecCqy1Wib6ZY8tAk9cQIN4J5CAh8a2rYicVrHNqprAsv3DqKljUuZN7S9Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**FCIP接口**



FCIP接口是指FCIP链路本地以太网接口和VE Port接口。所有FCIP和E Port配置都是正对FCIP接口。



FCIP参数包含以下：

1. Gigabit以太网口和TCP连接参数
2. 对端信息
3. FCIP链路TCP连接数量
4. E Port参数（trunking模式和trunk allowed VSAN列表）























## iSCSI, FC和FCoE的比较和适用场景

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-03-29*

iSCSI, FC和FCoE都是目前SAN网络上存储设备的主流连接方式，那这三种协议各自间的优缺点是什么，各自的适用场景如何？



FC是部署最多的SAN协议了，大家都很熟悉。iSCSI和FCoE都运行在以太网上，因此可以帮助企业节省IT架构的投入成本和复杂度。特别是iSCSI，可以直接沿用企业现有的IT架构，对很多中小型企业这是不可忽视的优势。

 

这三种协议工作在不同的网络层(见下图)。FCoE起步就是10Gb以太网；而iSCSI可以工作在1Gb或10Gb以太网；FC则有2GB、4Gb或8GB。另外iSCSI支持software initiator，普通台式机也可以接入存储。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUCcRXTLDHG91cHrVvicJq7vw6jicrtSiaxOoWMI6T1DbicAG6zhhHA14D6lRDVmicKgYq1ACBAYSEzwaw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)















## 网络虚拟化（一）：简介

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-05*

目前，软件定义的数据中心是一大热门技术，VMware作为全球最大的虚拟化厂商实现了通过软件可以定义应用及其所需的所有资源，包括服务器、存储、网络和安全功能都会实现虚拟化，然后组合所有元素以创建一个软件定义的数据中心。通过虚拟化可以减少服务器部署的时间和成本，可以实现灵活性和资源利用率的最大化，可以在调配虚拟机时对环境进行自定义，在软件定义的数据中心里虚拟机可以跨越物理子网边界。小编作为一名虚拟化技术爱好者，将通过系列文章来介绍软件定义数据中心里的网络虚拟化。

 

传统的网络在第2层利用VLAN来实现广播隔离，在以太网数据帧中使用12位的VLAN ID将第二层网络划分成多个广播域，VLAN数量需少于4094个。但随着虚拟化的普及，4094个的数值上限面临着巨大压力。此外，由于生成树协议（STP）的限制，极大的限制了可以使用的VLAN 数量。基于VXLAN的网络虚拟化解决了传统物理网络面临的诸多难题。

 

网络虚拟化可将网络抽象化为一个广义的网络容量池。因此便可以将统一网络容量池以最佳的方式分割成多个逻辑网络。您可以创建跨越物理边界的逻辑网络，从而实现跨集群和单位的计算资源优化。不同于传统体系架构，逻辑网络无需重新配置底层物理硬件即可实现扩展。VMware网络虚拟化是通过虚拟可扩展局域网（VXLAN）技术，创建叠加在物理网络基础架构之上的逻辑网络。



VMware网络虚拟化解决方案满足了数据中心的以下几大需求：

 

- 提高计算利用率
- 实现集群的扩展
- 跨数据中心内多个机架利用容量
- 解决IP寻址难题
- 避免大型环境中VLAN数量剧增问题
- 实现大规模多租户

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXic0tD4sox6x3RsVppkeMzkrEEf4gXZNJ922CoFicn6Z1GLia56pO8ibYXiaCVfbsS5x1nibsQIMH2akUw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

通过采用网络虚拟化，可以有效的解决这些问题并实现业务优势：

 

- 加快网络和服务的调配速度，实现业务敏捷性。
- 将逻辑网络与物理网络分离，提供充分的灵活性。
- 大规模隔离网络流量并将其分段。
- 自动执行可重复的网络和服务调配工作流。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXic0tD4sox6x3RsVppkeMzkMy1TfOwNFkLEDbB3IqM3SN9QVCvk5JvazqJXtgiadBnjUSjas3V1vqw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)















## 网络虚拟化（二）：虚拟交换机

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-06*

虚拟交换机作为虚拟网络连接的关键组件，可以让ESXi主机上的虚拟机在使用相同协议的情况下实现通信，并连接到外部网络。本文将介绍VMware虚拟交换机技术。

 

虚拟交换机在许多方面都与物理以太网交换机相似。每个虚拟交换机都是互相隔离的，拥有自己的转发表，因此交换机查找的每个目的地只能与发出帧的同一虚拟交换机上的端口匹配。它可以在数据链路层转发数据帧，然后通过以太网适配器出口连接到外部网络。虚拟交换机能够将多个以太网适配器绑定在一起，类似于传统服务器上的网卡绑定，从而为使用虚拟交换机提供更高的可用性和带宽。它还支持端口级别的VLAN分段，因此可以将每个端口配置为访问端口或中继端口，从而提供对单个或多个VLAN的访问。

 

但是与物理交换机不同，虚拟交换机不需要生成树协议，因为它强制使用单层网络连接拓扑。多个虚拟交换机无法进行互连，在同一台主机内，网络通信流量无法在虚拟交换机之间直连流动。虚拟交换机通过一个交换机提供用户需要的所有端口。虚拟交换机无需进行串联，因为它们不共享物理以太网适配器。

 



**虚拟交换机可提供二种与主机和虚拟机相连接的类型：**

 

- 将虚拟机连接到物理网络。
- 将VMkernel服务连接到物理网络。VMkernel服务包括访问IP存储（如：NFS或iSCSI）、执行vMotion迁移以及访问管理网络。

 

设计网络连接环境时，您可以通过VMware vSphere将所有网络都置于一个虚拟交换机中。或者，您也可以选择多个虚拟交换机，每个虚拟交换机具有一个单独的网络。具体作何选择在某种程度上取决于物理网络的布局。例如：您可能没有足够的网络适配器，无法为每个网络创建一个单独的虚拟交换机。因此，您可能会将这些网络适配器绑定在一个虚拟交换机上，然后使用VLAN来隔离这些网络。

 

 

**虚拟网络支持二种类型的虚拟交换机：**

 

- 虚拟网络标准交换机：主机级别的虚拟交换机配置。

 

标准交换机可以将多个虚拟机连接起来，并使它们彼此可以进行通信。每个标准交换机最多有4088个虚拟交换机端口，而每台主机最多有4096个虚拟交换机端口。下图显示了几个标准交换机的不同用途。这些交换机从左到右依次为：

 

1. 配置绑定网卡的标准交换机。绑定的网卡可自动分发数据包以及执行故障切换。
2. 仅限内部使用的标准交换机，允许单个ESXi主机内的虚拟机直接与其他连接到同一标准交换机的虚拟机进行通信。VM1和VM2可使用此交换机互相通信。
3. 配置一个出站适配器的标准交换机。该交换机提供VM3使用。
4. VMkernel用来实现远程管理功能的标准交换机。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXic0tD4sox6x3RsVppkeMzk0K2jntMNxpiaibiaDIHRGmC3NJgTRCXhCsJAgcatHsS4ZAAx9XXJFjXvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

- 虚拟网络分布式交换机：虚拟网络分布式交换机是一款数据中心级交换机。标准交换机基于主机工作，交换机配置保存在ESXi主机上面。而数据中心交换机能够独立于物理结构实现统一虚拟化管理。虚拟网络分布式交换机配置通过vCenterServer管理，并且所有虚拟网络配置的详细信息都存储在vCenter Server数据库中。VXLAN网络可在一个或多个vSphereDistributed Switch上进行配置。

 

另外，vNetwork分布式交换机具有以下特征：

 

1. 独立于物理结构的统一网络虚拟化管理。
2. 针对整个数据中心管理一台交换机与针对每台主机管理若干标准虚拟交换机。
3. 支持VMware vSpherevMotion，因此统计数据和策略可随虚拟机一同转移。
4. 独立的管理界面。
5. 高级流量管理功能。
6. 监控和故障排除功能，如NetFlow和端口镜像。
7. 主机级别的数据包捕获工具（tcpdump）。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXic0tD4sox6x3RsVppkeMzk64QS2mTNcJI4lUZRQbOlQr9l94ib5JGiaA46nkcW6gsQv4K9v0AWb3ZA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)













## 网络虚拟化（三）：VXLAN虚拟可扩展局域网（上）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-07*

在2011年的VMworld大会上，VMware提出了VXLAN（virtual Extensible LAN虚拟可扩展局域网）技术。VXLAN技术是VMware、CISCO、Arista、Broadcom、Citrix和Redhat等厂商共同开发用于虚拟网络的技术，与之抗衡的有Microsoft联合Intel、HP和Dell开发的NVGRE标准（Network Virtualization using Generic Routing Encapsulation）。本文将重点介绍VXLAN的优势、VMware的VXLAN组件和应用案例分析。



 

**VXLAN逻辑网络有以下几项优于传统物理网络的明显优势：**

 

**1、突破了传统VLAN的数量限制。**

物理网络使用VLAN来限制和隔离第2层广播域，VLAN的数量上限为4094个。随着主机虚拟化技术的兴起，4094个VLAN数已经远不能满足云数据中心的需求。不同于VLAN的4094限制，VXLAN网络可以支持多达1600万个VLAN标识符。

 

**2、突破了传统的物理界限，满足多租户环境和规模扩展的需求。**

VXLAN网络是一个创建叠加在物理网络基础架构之上的逻辑网络，实现了在底层硬件上的独立配置。VXLAN网络大大减少了数据中心网络管理和配置所花费的时间，它提供的多层次网络拓扑结构和企业级安全服务，可将部署、调配时间从几周减少到数小时。同时，在VXLAN网络部署的虚拟机可以实现跨物理机迁移，例如：北京数据中心的虚拟机可以和上海的数据中心的虚拟机在二层网络上进行通信，同时支持跨数据中心物理机迁移功能，打破了传统的二层网络的界限。

 

**3、解决STP（生成树协议）高负荷**

VXLAN 中使用了新技术替代STP（生成树协议）, 因此解决了汇聚层交换机由于STP高负荷导致的压力过大问题。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXic0tD4sox6x3RsVppkeMzk1E5iael7HWibG40QiaKDdEXmONmrUMW156TqwoqDicRC2jVl1lSouy0tlA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

 

**在vSphere 5.5版本中，VXLAN实现组件包括：**

 

**• vShield Manager**

vShield Manager是vShield的集中式网络管理组件，可作为虚拟设备安装在vCenter Server 环境中的任意ESX主机上。vShieldManager可在与安装vShield代理不同的ESX主机上运行。

使用 vShield Manager用户界面或vSphere Client插件，管理员可以安装、配置和维护vShield组件。vShield Manager可以定义并管理VXLAN网络，包括：定义VXLAN网络的延展范围、配置vSphere承载VXLAN网络的VDS和配置VTEP等。

 

**• vSphere分布式交换机**

在VXLAN网络中vSphere分布式交换机用于连接主机和互连。

 

**• vSphere主机**

在VXLAN网络中每台vSphere主机上需要配置虚拟安全加密链路端点（VETP）模块，每个主机VEP会分配到一个唯一的IP地址，在vmknic虚拟适配器上进行配置，用于建立主机之间的通信安全加密链路并承载VXLAN流量。VTEP由以下三个模块组成：

 

\1. vmkernel模块

此模块负责VXLAN数据路径处理，其中包括转发表的维护以及数据包的封装和拆封。

 

2、vmknic虚拟适配器

此模块用于承载VXLAN控制流量，其中包括对多播接入、DHCP和ARP请求的响应。

 

3、VXLAN端口组

此端口组包括物理网卡、VLAN信息、绑定策略等。端口组参数规定了VXLAN流量如何通过物理网卡进出主机VTEP。



 

**创建VXLAN虚拟网络案例演示**

 

此方案的情形如下：在数据中心的两个群集上有多个 ESX 主机。工程部门和财务部门都在Cluster1 上。市场部门在Cluster2 上。两个群集都由单个vCenter Server 5.5进行 管理。

 



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXic0tD4sox6x3RsVppkeMzkmXbMVMZhpHcSDvVKDO2pV0XuuTg5GE1nZ3VCo5yIgazY1yWkRfP5Fw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



 

Cluster1 上的计算空间不足，而 Cluster2 未充分利用。老板要求IT管理员将工程部门的虚拟机扩展到 Cluster2上，实现工程部门的虚拟机位于两个群集中，并且能够彼此通信。如果 IT管理员使用传统方法解决此问题，他需要以特殊方式连接单独的 VLAN 以便使两个群集处于同一二层域中。这可能需要购买新的物理设备以分离流量，并可能导致诸如 VLAN 散乱、网络循环以及系统和管理开销等问题。



通过 VXLAN技术，IT管理员可以通过跨dvSwitch1 和 dvSwitch2 构建VXLAN 虚拟网络，在不添加物理设备的情况下达到要求。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXic0tD4sox6x3RsVppkeMzkCYqUmtq67kEhibiaFzDVRIF0yOAEGKMcyz7xP71qeg6OIV0clibElC47w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)













## 网络虚拟化（四）：VXLAN虚拟可扩展局域网（下）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-08*

上篇文章《VXLAN虚拟可扩展局域网（上）》中介绍了VXLAN的优势、VMware环境中VXLAN的组件和应用案例，本文将分析VXLAN传输数据包、工作原理和案例解析。



**VXLAN传输数据包**

 

VXLAN虚拟可扩展局域网是一种overlay的网络技术，使用MAC in UDP的方法进行封装，在封装包中间添加了一层共50字节的VXLAN Header，然后以IP数据包的形式通过3层网络进行传输。位于VXLAN安全加密链路任何一端的虚拟机不知道这个封装包。同时，物理网络中的设备也不知道虚拟机的源或目的MAC或IP地址。VXLAN的封装结构如下图所示：



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXPfrGu4usibcicocjWFXZiaiaa1bVPQHemMQcDhl6eD0dia116dWceNfdQjoArlrEMrofIay1q4ctibiauQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**1. VXLAN Header：**

共计8个字节，目前被使用的是Flags中的一个标识位和24bit的VXLAN Network Identifier，其余的部分没有定义，但是在使用的时候必须设置为0×0000。

 

**2. 外层的UDP报头：**

目的端口使用4789，但是可以根据需要进行修改。同时UDP的校验和必须设置成全0。

 

**3. IP报文头：**

目的IP地址可以是单播地址，也可以是多播地址。

单播情况下，目的IP地址是VXLAN Tunnel End Point(VTEP)的IP地址。

在多播情况下引入VXLAN管理层，利用VNI和IP多播组的映射来确定VTEPs。

 

从封装的结构上来看，VXLAN提供了将二层网络overlay在三层网络上的能力，VXLAN Header中的VNI有24个bit，数量远远大于4096，并且UDP的封装可以穿越三层网络，因此比的VLAN更好的可扩展性。

 

 

**VXLAN协议网络工作原理**

 

**（1）、网络初始化**

 

在VXLAN协议工作前需要进行网络初始化配置。网络初始化就是让虚拟网络中的主机加入到该VXLAN网络所关联的多播组。例如：VM1和VM2连接到VXLAN网络，那么二台VXLAN主机（ESXi1和ESXi2）就需要先加入IP多播组239.119.1.1。VXLAN的网络标识符（VNI）就是网络ID。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXPfrGu4usibcicocjWFXZiaiaaUNpYuxJrOTKk5yuJTacXk9uWNric707FyP6Lqc2qUSzPtN0HziaztTJQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**（2）、ARP查询**

 

下图描述了VXLAN协议中二个连接到逻辑2层网络的虚拟机（VM1和VM2）ARP查询流程。

 

\1.   VM1以广播形式发送ARP请求；

\2.   VTEP1封装报文。本例中，VXLAN 100关联在IP多播组239.119.1.1中，VNI为100；

\3.   VTEP1通过多播组将数据包发送给VTEP2；

\4.   VTEP2接收到多播包。VTEP2将验证VXLAN网段ID，拆封数据包，然后将通过2层广播包的形式其转发到虚拟机VM2；

\5.   VM2收到广播包后发送ARP响应。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXPfrGu4usibcicocjWFXZiaiaaP2NRQNVmt89Zjdsdic0FwiboKFYYxxAudNAIsqQMJlL9wSD7J5CNjibXQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



注意：VTEP1只会在VTEP转发表中没有虚拟机MAC与该MAC地址的VTEP IP之间的映射时，才会生成多播包。在广播数据包时，如果MAC转发表中没有与帧目的MAC地址相匹配的条目，2层交换机会执行ARP查询操作。在发现虚拟机MAC地址与VTEP IP地址的映射条目并将其更新到转发表中后，任何与该特定虚拟机通信的请求都将通过点到点安全加密链路传输。

 

**（3）、ARP应答**

 

ARP应答处理流程类似于ARP请求，不同之处在于VM2将通过单播包进行ARP响应。因为VTEP2已经获得了VM1的MAC地址、IP地址以及VTEP1的信息。VTEP2将建立一个转发条目，以后交换数据包操作会使用该转发条目。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXPfrGu4usibcicocjWFXZiaiaagTXxKIwMEIwYyQDu5NU6X1HicI8LfVpkicyMxa1xcjuSWXftCSDib6Y1Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**（4）、VXLAN网关**

如果需要VXLAN网络和非VXLAN网络连接，必须使用VXLAN网关才能把VXLAN网络和外部网络进行连接。下图描述了VXLAN网关的工作原理：

 

\1.   VM2通过网关MAC地址向网关发送数据包；

\2.   VTEP2封装数据包，通过多播（第一次）发送给VTEP1;

\3.   VTEP1拆封数据包，并发送到网关；

\4.   网关将IP数据包路由到Internet。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXPfrGu4usibcicocjWFXZiaiaaN1HTibPicGNtGeI3a2xUSt6q4PaeRC4JazTCyfn0ibpISjGgRJpPsEqCA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



 

**案例（一）**

 

当二台虚拟机在同一逻辑2层网络中时，如果二个虚拟机都在同一台vSphere主机上，那么数据包无需封装。如果二个虚拟机在不同vSphere主机上，一台vSphere主机上的源VTEP将虚拟机数据包封装到一个新UDP标头中，并通过外部IP网络将其发送到另一台vSphere主机上的目标VTEP。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXPfrGu4usibcicocjWFXZiaiaakZs3nqaQA2eGykS512iciaib7YeWibbgLXbQGylIbZRhhV5dGmGpavDhIw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 



**案例（二）**

 

图中显示了二个虚拟网络VXLAN-A和VXLAN-B。二个网络分别属于192.168.1.0/24网段和192.168.2.0/24网段，二个网络通过VXLAN网关进行连接。以下是可能情况：

 

（1）、当所有虚拟机和VXLAN网关位于同一vSphere主机上时。虚拟机将流量导向各自逻辑网络子网的网关IP地址，VXLAN会根据防火墙规则在二个不同接口之间进行路由。

（2）、当所有虚拟机不在同一台vSphere主机上，而VXLAN网关部署在其中一台vSphere主机时。虚拟机的流量将被封装到数据包，然后进过物理网络传送到VXLAN网关，之后将由网关将数据包路由到正确的目标。

（3）、当所有虚拟机和VXLAN网关不在同一台vSphere时。数据包传输将类似于情况2。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXPfrGu4usibcicocjWFXZiaiaanCPIwSZuTgEzSDd7KsF3pia3Su6sGYibt7P3IyRosmVntiaq5MCVHVKOw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)















## 网络虚拟化（五）：通过划Zone来提高虚拟网络的安全性

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-09*

虚拟网络为数据中心的建设和运营提供了显著优势，但同时也带来了新的安全问题。本文将介绍通过划Zone来提高虚拟网络的安全性。

 

虚拟环境面临的最常见威胁是不安全的接口和网络、过高的权限、错误配置或不当管理，以及未打补丁的组件。由于虚拟机是直接安装在服务器硬件上的，因此许多常规安全漏洞并不存在太大的安全威胁。在vSphere环境中，必须保护好以下基本组件：

 

- 物理网络和虚拟网络
- 基于IP的存储和光纤通道
- 物理和虚拟应用服务器以及应用客户端
- 托管虚拟机的所有ESXi系统
- 数据中心内的所有虚拟机
- 虚拟机上运行的应用程序

 

划Zone是保护物理网络和虚拟网络的一种有效方法。Zone定义了一个网段，在网段中的数据流入和流出都将受到严格的控制。在虚拟网络中，常见的划Zone方式有以下三种：

 

 

**1、通过物理设备实现分离**

 

在这种配置中，每个区域都采用单独的ESXi物理主机或集群，区域隔离是通过服务器的物理隔离和物理网络安全设定来实现的。这种配置方法较为简单，复杂度较低，不需要对物理环境进行调整，是一种将虚拟技术引入网络的好办法。但是，这种配置方法会制约虚拟化提供的优势，资源整合率和利用率较低，使用成本较高。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUibrRHjaF5ThBKBDKPLngdyEdeLCHcNbvC47uTf2pOr73TBypA9XxMkdKvibCL71Uqlwk1vp4qxgyQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**
**

**2、通过虚拟技术实现分离**

 

在这种配置中，通过使用虚拟交换机可以将虚拟服务器连接到相应的区域，在同一台ESXi主机上设置不同信任级别的虚拟服务器，这种区域是在虚拟化层中实施的。虽然这种方法可以实现在物理机和虚拟领域实施不同的安全设定，但是仍然需要通过物理硬件来确保区域之间的网络安全。虽然在每个区域中都显示了不同的虚拟交换机，但是用户仍然可以使用VLAN以及单个虚拟交换机上不同的端口组实现相同的目的。

 

这种方法较好的整合了物理资源，能较好地利用虚拟化优势，成本较低。然而，与采用物理设备实现分离相比，这种配置较为复杂，必须明确配置人员，需要定期审核配置。

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUibrRHjaF5ThBKBDKPLngdydu0MQic1ia9Pvhrb8HXINRdDvuZibT77Ix9xN0myB255OvSAuCdiaAXlcg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**
**

**3、完全合并再分离**

 

这是一种建立在完全虚拟前提下的隔离。用户可以将不同安全级别的虚拟机置于不同物理服务器上，并将网络安全设备引入虚拟基础架构。通过虚拟网络设备实现管理和保护虚拟信任域之间的通行。例如：通过VMware的vShield组件，可以为不同区域建立通信，同时监控通信。

 

这种配置中，所有的服务器和安全设备都进行了虚拟化，用户可以隔离虚拟服务器和网络，使用虚拟安全设备管理不同区域之间的通信。这是配置能够充分利用资源，减低成本，通过单个管理工作站管理整个网络，但是配置和管理最为复杂，出错几率较高。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUibrRHjaF5ThBKBDKPLngdyBicBQ0RNkbSBu1u6RuRvcQeo7kncFKuwod4PoUlS0YwHdObHkhX21ug/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)















## SMI-S协议简介

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-03-31*

SMI-S（Storage Management Initiative Specification存储管理主动）是SNIA（全球网络存储工业协会）发起并主导，众多存储厂商共同参与开发的一种标准管理接口。其目标是在存储网络中的存储设备和管理软件之间提供标准化的通信方式，从而使存储管理实现厂商无关性，提高管理效率、降低管理成本，促进存储网络的发展。本文作为存储入门基础知识介绍的一部分，将为大家介绍SMI-S协议的相关知识。

 

SMI-S是建立在一些已有的标准基础上，主要是CIM（Common Information公共Model信息模型）和WBEM（Web-Based Enterprise Management基于Web的企业管理）。

 

WBEM是由DMTF（分布式管理工作组）负责开发的一套使用Internet 标准技术的一体化企业计算管理环境。它提供了基于WEB技术的完整的工业统一管理工具。促进了完全不同技术和平台的数据交换。

 

CIM为WBEM的核心定义了一种分层次的、面向对象的信息模型和架构，该架构可以为企业网络管理整个范围内的系统，网络，应用程序和服务的信息管理提供公共定义，而且允许用户扩充。它描述了管理的概念模型，可以使得用户可以通过网络在彼此的系统之间交换语义丰富的管理信息。

 

 

**发展历程**

 

2002年，SNIA开始主推SMI-S（SMI Specification，SMI计划的相关技术标准），希望对存储网络的管理提供一个统一的标准，这也成为业界为存储管理标准化所做的首次尝试。

2003年，SNIA公布了SMI-S 1.0版本规范，并且致力于SMI-S规范支持度的测试和认证工作。

2004年，SMI-S 1.0.2成为ANSI标准。

2005年，SMI-S 规范提交ISO审核，1.10 版release。

2007年，SIM-S 规范正式通过ISO认证，成为国际性的存储管理标准。

SMI-S发布至今，已经升级到了1.6版本，并且取得多家SNIA成员企业（诸如IBM、HP、EMC等）的认可与支持。

 



**开源服务框架**

 

Open Pegasus是CIM与WBEM标准的开源实现。它由C++ 编写，所以可以很方便的将面向对象的CIM管理对象转变成程序模块。因此他被各个操作系统平台所支撑，包括UNIX, Linux, OpenVMS, and Microsoft Windows。



WBEM Services是用Java编写的，适用于任何商业和非商业用途的，基于ＷＢＥＭ的实现。它的内容包括运用编程接口，服务器端和客户端的运用程序和工具。



SFCB (Small Footprint CIM Broker) 是一个轻量级的CIMOM，十分适用于嵌入式环境的开发。

 

 

**客户端**

 

StorageIM是一个基于网页的存储管理系统。它可以自动发现满足CIM和SMI-S管理标准的系统并报告这些系统的存储状况。



CIMNavigator 是一个基于Java的图形管理工具，能够管理本地或者远端的被CIMOM管理的CIM对象。

 

 

**自动化测试工具**

 

SNIA-CTP（Conformance Testing Program）官方标准测试的工具。SNIA有自己的工作室ICTP（Interoperability Conformance Test Program）专门负责SMI-S支持度测试。



SMI-S Test Tool　一个第三方的SMI-S规范测试工具。

 

 

**设计目的**

  

  一方面，SMI-S为存储管理提供了一个统一的理界面，使用户能够在SAN中轻松的集成和管理来自多个厂商的产品，从而提升了灵活性、可管理性和可靠性；同时，用户的资源利用率也将获得极大的提高。



  另一方面，它为网络存储行业定义了一个全新、开放的开发模式，使存储厂商能够专注于附加值功能上，而省去了异构和专有接口开发整合所需的技术支持。SMI-S在统一理解存储管理上对所有厂商都是至关重要的。有了一个公共平台，厂商就可以加速产品的开发进程，并且终端用户可以更自由地选择厂商，同时也降低了复杂性。

​    

 

**相关联接**

 

SMI-Shttp://www.snia.org/forums/smi/tech_programs/smis_home

CIM http://www.dmtf.org/standards/cim/

WBEMhttp://www.dmtf.org/standards/wbem/















## iSCSI存储系统基础知识（一）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-11*

iSCSI是由Cisco和 IBM两家发起的，2003年2月由IETF（互联网工程任务组）认证通过，是一项比较成熟的技术。它将SCSI命令封装在TCP/IP包里，并使用一个iSCSI帧头。它基于IP协议栈，假设以不可靠的网络为基础，依靠TCP恢复丢失的数据包。



本些列作为iSCSI系统的基础读物，覆盖以下内容：1. 什么是iSCSI？2. 它有哪些优点？3. 它的发展现状如何？4. 它的架构和组件是怎样的？5. 它的部署模式如何？

 

**背景:**

 

相比直连存储，网络存储解决方案能够更加有效地共享，整合和管理资源。从服务器为中心的存储转向网络存储，一直依赖于数据传输技术的发展，速度要求与直连存储相当，甚至更高，同时需要克服并行SCSI固有的局限性。

 

所有数据在没有文件系统格式化的情况下，都以块的形式存储于磁盘之上。并行SCSI将数据以块的形式传送至存储，但是，对于网络它的用处相当有限，因为线缆不能超过25米，而且最多只能连接16个设备。

 

光纤通道是目前SAN的主导架构，它在专门的高速网络上分离存储资源。光纤通道协议与互联技术起源于满足高性能传送块数据的需求，同时克服直连存储的连接和距离限制。通常光纤通道设备连接距离可达到10000米，甚至数十万米，并且对于连接在SAN之上的设备没有数量要求。

 

与SAN不同，NAS将数据以文件的形式传输并且可以直接连接至IP网络。部署NAS设备传送数据库块数据，使用基于SCSI的光纤通道协议比Server Message Block(SMB)协议更加高效。

 



**什么是iSCSI：**

 

iSCSI是一种使用TCP/IP协议在现有IP网络上传输SCSI块命令的工业标准，它是一种在现有的IP网络上无需安装单独的光纤网络即可同时传输消息和块数据的突破性技术。iSCSI基于应用非常广泛的TCP/IP协议，将SCSI命令/数据块封装为iSCSI包，再封装至TCP 报文，然后封装到IP 报文中。iSCSI通过TCP面向连接的协议来保护数据块的可靠交付。由于iSCSI基于IP协议栈，因此可以在标准以太网设备上通过路由或交换机来传输。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XZy876H203k9x4LB2atNvfibJFxP7x0JpWjyRicJalMF1AOXlZm7kUmdQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

iSCSI架构依然遵循典型的SCSI模式：随着光纤通道的发明initiator和target之间的SCSI线缆已被光线线缆所代替。现在随着iSCSI的出现光纤线缆又被价格低廉的网线和TCP/IP网络所替代。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016X1cicqw0G1MoCUTtS0Y5IMEBsUnGAxibMqDBug6icpSlNzcRlPIUH7TgYA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

虽然现有的光纤存储网络具有高吞吐量的优势，与其他厂商之间的互通性仍是一个短板。而基于成熟的TCP/IP协议的iSCSI网络，不仅免于互通性限制而且在安全性等方面具备优势。同时，由于千兆以太网的增量部署，iSCSI的吞吐量也会随之增加，与光线通道匹敌甚至超越光线通道。

 



**iSCSI的优势：**

 

iSCSI的优势包括：

 

**长距离连接**：SAN网络集中管理存储资源，能够覆盖一个市区范围。对于分布在海外的组织则面临一系列未连接的“SAN孤岛”，当前的光纤通道连接受限于10km而无法桥接。有扩展的光纤通道连接可达数百公里，但这些方法既复杂又昂贵。广域网iSCSI (wide area network, WAN)提供了一种经济的长距离传输，可用于目前FC SAN或iSCSI SAN的桥接。

 

**更低成本**：不同于FC SAN解决方案需要部署全新的网络基础架构，并且需要专业技术知识，专门的硬件故障排查，iSCSI SAN解决方案充分利用了现有的局域网基础设施，使之可广泛应用于大多数组织。

 

**简化部署和实施**：iSCSI解决方案仅需要在主机服务器上安装iSCSI initiator，一个iSCSI target存储设备，一个千兆比特以太网交换机以在IP网络传输块数据。诸如存储配置，调配，备份这样的管理操作可由系统管理员处理，与管理直连存储方式相同。而像集群这样的解决方案使用iSCSI也比FC更为简易。

 

**固有的安全性**：光纤通道协议并没有内嵌的安全保障。取而代之，通过对SAN的物理连接限制来保障安全。虽然对于被限制在加锁的数据中心的SAN来说是有效的，但随着FC协议变得更加广为人知以及SAN开始连接到IP网络，这种安全方法已失去其功效。

 

相比之下，微软实施的iSCSI协议使用质询握手身份验证协议（CHAP）进行验证和Internet协议安全（IPSec）标准加密为网络上的设备提供安全保障。目前，iSCSI target实现了CHAP，但暂时没有更加先进的方法。

 



**iSCSI的现状与挑战：**

 

iSCSI这几年来得到了快速发展，近两年iSCSI的热度持续走高，各存储设备厂商纷纷推出iSCSI设备，销量也在快速增长。基于iSCSI的SAN现在已经相对成熟。10Gbps以太网的出现极大地改变了iSCSI的传输速率，大多数应用的响应能力完全可以适应用户的需求。同时iSCSI产品的采购成本与维护成本都比FC要低。



但是，iSCSI仍受到几个掣肘：iSCSI架构于IP协议之上，因此也继承了IP协议本身的缺陷：区分不同流量优先等级，防止丢包的QoS与流量控制机制不足，而FCoE在这一点上，具备暂停帧需求和将高优先级流量先于低优先级流量传输的功能，无疑具有先天的优势。即使带宽提升到10Gb，TCP/IP协议管理方面的问题在仍会影响iSCSI的效率表现。 此外，以太网带宽扩展到10Gb只是外部传输通道的增加，如果主机I/O处理能力、存储阵列性能无法跟上，则存储网络整体性能同样会受到影响。目前iSCSI在高I/O环境下的性能表现仍不如光纤通道。













## iSCSI存储系统基础知识（二）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-13*

 

**iSCSI SAN概念解析:**





iSCSI SAN组件与FC SAN组件相类似。包括以下部件：

 

**iSCSI Client/Host：**

 

系统中的iSCSI客户端或主机（也称为iSCSI initiator），诸如服务器，连接在IP网络并对iSCSI target发起请求以及接收响应。每一个iSCSI主机通过唯一的IQN来识别，类似于光纤通道的WWN。

 

要在IP网络上传递SCSI块命令，必须在iSCSI主机上安装iSCSI驱动。推荐通过GE适配器（每秒1000 megabits）连接至iSCSI target。如同标准10/100适配器，大多数Gigabit适配器使用Category 5 或Category 6E线缆。适配器上的各端口通过唯一的IP地址来识别。





**iSCSI Target：**

 

iSCSI target是接收iSCSI命令的设备。此设备可以是终端节点，如存储设备，或是中间设备，如IP和光纤设备之间的连接桥。

 

每一个iSCSI target通过唯一的IQN来标识，存储阵列控制器上（或桥接器上）的各端口通过一个或多个IP地址来标识。





**本机与异构IP SAN：**

 

iSCSI initiator与iSCSI target之间的关系如图1所示。本例中，iSCSI initiator(或client)是主机服务器而iSCSI target是存储阵列。此拓扑称为本机iSCSI SAN,它包含在TCP/IP上传输SCSI协议的整个组件。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XowsfewwvqhKT0Jok3n9xcT5oNQkDPyMKiboST3lXibiafyoY4tHGZM46A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

与之相反，异构IP SAN，如下图所示，包含在TCP/IP与光纤交换结构传输SCSI的组件。为了实现这一点，需要在IP与光纤通道之间安装连接桥或网关设备。连接桥用于TCP/IP与光纤通道之间的协议转换，因此iSCSI主机将存储看做iSCSI target。直接连接光纤通道target的服务器必须包含HBA而不是iSCSI主机的网络适配卡。iSCSI主机可使用NIC或HBA。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XZibMw91PZUhsrZeE733yMkgaVjEQreV8buibaCvI9rjNQBbdktePeqow/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 



**iSCSI 存储系统四大架构:**



**控制器系统架构：**

 

iSCSI的核心处理单元采用与FC光纤存储设备相同的结构。即采用专用的数据传输芯片、专用的RAID数据校验芯片、专用的高性能cache缓存和专用的嵌入式系统平台。打开设备机箱时可以看到iSCSI设备内部采用无线缆的背板结构，所有部件与背板之间通过标准或非标准的插槽链接在一起，而不是普通PC中的多种不同型号和规格的线缆链接。

 

控制器架构iSCSI存储内部基于无线缆的背板链接方式，完全消除了链接上的单点故障，因此系统更安全，性能更稳定。一般可用于对性能的稳定性和高可用性具有较高要求的在线存储系统，比如：中小型数据库系统，大型数据的库备份系统，远程容灾系统，网站、电力或非线性编辑制作网等。





**连接桥系统架构：**

**
**

整个iSCSI存储系统架构分为两个部分，一个部分是前端协议转换设备，另一部分是后端存储。结构上类似NAS网关及其后端存储设备。

 

前端协议转换部分一般为硬件设备，主机接口为千兆以太网接口，磁盘接口一般为SCSI接口或FC接口，可连接SCSI磁盘阵列和FC存储设备。通过千兆以太网主机接口对外提供iSCSI数据传输协议。

 

后端存储一般采用SCSI磁盘阵列和FC存储设备，将SCSI磁盘阵列和FC存储设备的主机接口直接连接到iSCSI桥的磁盘接口上。

 

iSCSI连接桥设备本身只有协议转换功能，没有RAID校验和快照、卷复制等功能。创建RAID组、创建LUN等操作必须在存储设备上完成，存储设备有什么功能，整个iSCSI设备就具有什么样的功能。

 

**PC系统架构：**

 

即选择一个普通的、性能优良的、可支持多块磁盘的PC(一般为PC服务器和工控服务器)，选择一款相对成熟稳定的iSCSI target软件，将iSCSI target软件安装在PC服务器上，使普通的PC服务器转变成一台iSCSI存储设备，并通过PC服务器的以太网卡对外提供iSCSI数据传输协议。

 

在PC架构的iSCSI存储设备上，所有的RAID组校验、逻辑卷管理、iSCSI 运算、TCP/IP 运算等都是以纯软件方式实现，因此对PC的CPU和内存的性能要求较高。另外iSCSI存储设备的性能极容易收PC服务器运行状态的影响。

 

**PC+NIC系统架构**

 

PC+iSCSI target软件方式是一种低价低效比的iSCSI存储系统架构解决方案，另外还有一种基于PC+NIC的高阶高效性iSCSI存储系统架构方案。

 

这款iSCSI存储系统架构方案是指在PC服务器中安装高性能的TOE智能NIC卡，将CPU资源较大的iSCSI运算、TCP/IP运算等数据传输操作转移到智能卡的硬件芯片上，由智能卡的专用硬件芯片来完成iSCSI运算、TCP/IP运算等，简化网络两端的内存数据交换程序，从而加速数据传输效率，降低PC的CPU占用，提高存储的性能。

















## iSCSI存储系统基础知识（三）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-14*

**协议映射:**

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XWaDAKn7BNQiaTCPozfPg4lFJoghicHiceHJ7iaEIshj0PaYibsQk4w4vy1Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

iSCSI协议是让SCSI协议在TCP协议之上工作的传输协议，是一种SCSI远程过程调用模型到TCP协议的映射。SCSI命令加载在iSCSI请求之上，同时SCSI状态和响应也由iSCSI来承载。iSCSI同样使用请求响应机制。在iSCSI 配置中，iSCSI 主机或服务器将请求发送到节点。 主机包含一个或多个连接到IP 网络的启动器，以发出请求，并接收来自iSCSI 目标的响应。 为每个启动器和目标都指定了一个唯一的iSCSI 名称，如 iSCSI 限定名 (IQN) 或扩展的唯一标识(EUI)。 IQN 是 223 字节的ASCII 名称。EUI 是 64 位标识。iSCSI 名称代表全球唯一命名方案，该方案用于标识各启动器或目标，其方式与使用全球节点名(WWNN) 来标识光纤通道光纤网中设备的方式相同。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XHY00m1lZkSFpzypICFSSibvDNB0Ia6akibrBviaEMLXA50QicyM8paPVtw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

iSCSI 目标是响应 iSCSI 命令的设备。iSCSI 设备可以是诸如存储设备的结束节点，或者可以是诸如IP 与光纤通道设备之间的网桥的中间设备。每个iSCSI 目标由唯一的iSCSI 名称标识。



要通过 IP 网络传输 SCSI 命令，iSCSI 驱动程序必须安装到iSCSI 主机和目标中。驱动程序用于通过主机或目标硬件中的网络接口控制器(NIC) 或 iSCSI HBA 来发送iSCSI 命令和响应。



为实现最佳性能，请使用传输速度为每秒 1000 兆位 (Mbps) 的千兆以太网适配器在iSCSI 主机和 iSCSI 目标间进行连接。

 

**iSCSI 命令封装：**

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XxRAicnrhIUQcwxYelqHhYIPbNvahNC7fLLJAB9ibCXkww5JhpXPtJsow/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

发起端和目标端之间以消息的形式进行通信。PDU（Protocal Data Unit）就是用来传输这些消息的。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XZz0hA7IibiaA5ibYrMBPnYPzJhdFrXbgg2EqKWHuYAmXfhyK1VbCJibdibg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

iSCSI 协议就是一个在网络上封包和解包的过程，在网络的一端，数据包被封装成包括TCP/IP头、iSCSI识别包和SCSI数据三部分内容，传输到网络另一端时，这三部分内容分别被顺序地解开。iSCSI 系统由一块 SCSI 卡发出一个 SCSI 命令，命令被封装到第四层的信息包中并发送。

 

接收方从信息包中抽取SCSI 命令并执行，然后把返回的SCSI命令和数据封装到IP信息包中，并将它们发回到发送方。系统抽取数据或命令，并把它们传回SCSI子系统。所有这一切的完成都无需用户干预，而且对终端用户是完全透明的。 为了保证安全，iSCSI 有自己的上网登录操作顺序。在它们首次运行的时候，启动器(initiator)设备将登录到目标设备中。

 

任何一个接收到没有执行登录过程的启动器的iSCSI PDU目标设备都将生成一个协议错误，而且目标设备也会关闭连接。在关闭会话之前，目标设备可能发送回一个被驳回的iSCSI PDU。这种安全性是基本的，因为它只保护了通信的启动，却没有在每个信息包的基础上提供安全性。还有其他的安全方法，包括利用IPsec。在控制和数据两种信息包中，IPsec 可以提供整体性，实施再次(replay)保护和确认证明，它也为各个信息包提供加密。

 

**iSCSI 会话：**

 

iSCSI 会话建立于一个initiator与一个target之间，一个会话允许多个TCP连接，并且支持跨连接的错误恢复。大多数通信还是建立在SCSI基础之上的，例如，使用R2T进行流量控制。iSCSI添加于SCSI之上的有：立即和主动的数据传输以避免往返通信；连接建立阶段添加登录环节，这是基于文本的参数协商。建立一个iSCSI会话，包括命名阶段：确定需要访问的存储，以及initiator，与FC不同，命名与位置无关；发现阶段：找到需要访问的存储；登录阶段：建立于存储的连接，读写之前首先进行参数协商，按照TCP连接登录。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016X8sFZK0XmiaDrcbA32uiba38OG1LW7jdvibwlQzcgTZibKCWWt8EuHUggSw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XkDWpmu1ibr1orEHsvw4BeAqyhzRt8sH3zL6kXAPtkhuTUUX9NHtDVLA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**结构模式:**

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XWoO86zbSujsezbvI5134yERNT4RlvkjlibLBNwxySMek4utErg9XrMA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

iSCSI有两大主要网络组件。第一个是网络团体，网络团体表现为可通过IP网络访问的一个驱动或者网关。一个网络团体必须有一个或者多个网络入口，每一个都可以使用，通过IP网络访问到一些iSCSI节点包含在网络团体中。第二个是网络入口，网络入口是一个网络团队的组件，有一个TCP/IP的网络地址可以使用给一个iSCSI节点，在一个ISCSI会话中提供连接。一个网络入口在启动设备中间被识别为一个IP地址。一个网络入口在目标设备被识别为一个IP地址+监听端口。

 

**iSCSI端口组:**

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XOy3viaPWLTxm22SDPMt5fclkWRf8BhXxwGTrdcHShFAFvNYfSB7D4xQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

iSCSI支持同一会话中的多个连接。在一些实现中也可以做到同一会话中跨网络端口组合连接。端口组定义了一个iSCSI节点内的一系列网络端口，提供跨越端口的会话连接支持。

















## Fibre Channel光纤通道系统基础（一）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-21*

光纤通道技术（Fibre Channel）是一种网络存储交换技术，可提供远距离和高带宽，能够在存储器、服务器和客户机节点间实现大型数据文件的传输。了解光纤通道技术是了解网络存储的起点。本系列文章全面介绍光纤通道系统的基础知识。内容包括Fibre Channel基本概念和优势，术语解释，拓扑结构。作为存储爱好者的入门读物。

 

**Fibre Channel的概念:**

 

Fibre Channel (FC) 是一种高速网络互联技术（通常的运行速率有2Gbps、4Gbps、8Gbps和16Gbps），主要用于连接计算机存储设备。过去，光纤通道大多用于超级计算机，但它也成为企业级存储SAN中的一种常见连接类型。尽管被称为光纤通道，但其信号也能在光纤之外的双绞线上运行。



光纤通道协议（Fibre Channel Protocol，FCP）是一种类似于TCP的传输协议，大多用于在光纤通道上传输SCSI命令。



光纤通道广泛用于通信接口，并成为传统I/O接口与网络技术相结合趋势的一部分。Network运作于一个开放的，非结构化的并且本质上不可预测的环境。Channels通常运行在一个封闭的、结构化的和可预测的环境，该环境下所有与主机通信的设备都预先已知，任何变更都需要主机软件或配置表进行相应更改。通道协议如SCSI，ESCON, IPI。Fibre Channel将这两种通信方式的优势集合为一种新的接口，同时满足network和channel用户的需求。

 

**Fibre Channel的目标与优势:**

 

Fibre Channel要提供的是一个连接计算机和共享外围设备的接口，在这一技术提出之前是通过多种不同的接口来连接的，如IDE，SCSI，ESCON。



Fibre Channel需要提供大量信息的高速传输。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfuNveMkDQePCmuvrBrOarMNh15BJNozG92xsI8ia1LVzsJ5vgm9bdcYoA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

上图显示了2Gbps Fibre Channel与Escon和SCSI同等级下的传送速率对比。

 

除了速度增长以外，Fibre Channel也需要支持公里级的距离。通过光纤交换机实现，如下图所示：

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfuhvbwRiaaB9HFF5V7O7wTgLKsbloqGnu8MOnAefZg9KHFMeNibeG7b3HA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



光纤通道还需要提供传输多种上层协议的能力，并维持这些上层协议的持续使用。光纤通道接口如下图所示：

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfuBKbOrBjQ3LcVrQ996dIufVfA8ukEHKGgpNkTibzic5BtL7IvJr8aDkDA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



连接和扩展是光纤通道的一个主要目标，通过将数千个设备共享数据并连接在一起来实现。Fibre Channel支持交换光纤，一个光纤结构理论上可支持一千六百万地址。光纤结构可以从一个单一交换机开始，按照需求可添加更多交换机来实现扩展。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfuu2d8YickP2nvFBkg0y6ibKuKPq8EUvwdJpiahKGYeeXJ82RsxZY4OYXNw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

光纤通道还需要提供比例如SCSI形式更简单的线缆和插头。光纤线缆比传统SCSI铜线更易于管理，插头体积更小从而一个适配器端口密度更高。当使用光纤线缆时，系统安装更为简便。下图显示了Fibre Channel使用的LC插头。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfu1MX9aG8BnMKMCpiaOHyKGRJAeolocoh5QQXaKyflOBgDvPVbefyFxrg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

无中断安装和服务也是光纤线缆的一个要求。不同于铜线，在插拔时需要断电，光纤在上下电时无需担心瞬态损伤。



可靠性，可用性和可维护性一直是光纤通道协议的目标。与铜线相比它具有明显的优势：对电磁干扰和线间串扰不明显。

















## Fibre Channel光纤通道系统基础（二）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-22*

光纤通道技术（Fibre Channel）是一种网络存储交换技术，可提供远距离和高带宽，能够在存储器、服务器和客户机节点间实现大型数据文件的传输。了解光纤通道技术是了解网络存储的起点。本系列文章全面介绍光纤通道系统的基础知识。内容包括Fibre Channel基本概念和优势，术语解释，拓扑结构。作为存储爱好者的入门读物。

 

本文作为系列之二，介绍光纤通道技术中的关键术语如：节点，端口，光纤，单模和多模，网络，拓扑。

 

**节点（Node）:**

 

光纤通道环境包括两个或更多通过互联拓扑连接在一起的设备。光纤通道设备如个人电脑，工作站，磁盘阵列控制器，磁盘和磁带设备都被称为节点。每个节点都是一个或多个节点的信息源或目的。以EMC为例节点可以是Symmetrix系统。每个节点需要一个或多个端口作为节点间通信的物理接口。端口是一个允许节点通过物理接口发送或接收信息的硬件附件。一些设备将这些端口集成，其他一些设备则使用可插拔端口如HBA。以EMC为例端口可以是Symmetrix FA适配器上的接口。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfulVZ4wlg1DY5yvKAjykf3QSxtBFodeVznPuFrfPhicJKAl2eY2szw66w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

 

**端口（Ports）:**

 

每一个光纤通道节点包含至少一个硬件端口，将该节点与光纤通道环境连接，并处理与其他端口的信息。此端口称为节点端口。一个节点可以有一个或多个节点端口。按照端口支持的协议标准有以下几种不同类型的节点端口：

 

N_PORT：Node_ports既可以用在端到端也可以用在光纤交换环境。在端到端环境下N_ports发送端与接收端之间直接互连。举例来说，一个HBA或一个Symmetrix FA端口就是一个N_port。

 

F_PORT：Fabric_Ports用于光纤交换环境下N_port之间的互连，从而所有节点都可以相互通信。通常这些端口在交换机上，允许HBA和其他设备如Symmetrix FA连接到光纤。

 

NL_PORT：NL_Port是支持仲裁环路的节点端口。例如，NL_Port可以是HBA或Symmetrix FA端口。

 

FL_PORT：FL_PORT是支持仲裁环路的交换端口。通常是交换机上连接到仲裁环路的端口。

 

E_PORT：E_Port是一个光纤扩展端口，用于在多路交换光纤环境下。E_ports通常指一个交换机上连接到光纤网络另一个交换机的端口。

 

G_PORT：G_Port是一个既能配成E_Port又能配成F_Port的通用端口。是一种位于交换机上的端口。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfuSwyg2U3RbH1HbfhAksAAsblLHzFfhwQ5Axibiab7hI1SgnNQFiaII4GvA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**光纤（Fiber）：**

 

端口通过链路连接至光纤网络。此链路包括线缆和承载两个独立光纤网络间收发信息端口的其他连接器。链路可能包括光导纤维或电缆。发送信号可能是长波光，短波光，LED或电子信号。

 

光纤结构包括光传输的纤芯。纤芯包裹着覆层，功能是反射并控制光在芯内传输。纤芯和覆层由玻璃材质制造并且很容易被损坏。为了保护光纤避免受到物理损坏覆盖了更多保护层，以使光纤能够承受一定力度。并有一个光纤可弯曲的最小角度，在这个角度附近光纤将被弯曲，超过这个角度将会导致光纤传输信号衰减，最坏情况将导致光纤损坏。正常使用下线缆较为坚实并且除了要留意最小弯曲半径以外无需特别维护。芯径和外径（µm为单位）通常是光纤规格的定义方式。例如，62.5/125µm光纤，芯径为62.5µm外径为125µm。两根这样的光纤结合在一根双芯线缆中，两端有相应的双芯连接器。两根光纤以相反的方向发送和接收数据。双芯线缆允许同步发送和接收。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfuUdYZ99UFO0zYvYPksARGPcAzOV7ibYpA4AhNg10yT1YpyJLEZjyCNiag/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**单模和多模（Single Mode and Multimode）：**

 

光纤通道中有两种传输模式。

 

单模链路的芯径为9-10µm并且使用位于光谱红外部分约为1300纳米的长波光作为光源。此光对于人眼是不可见的。下芯径允许单模链路支持端口间最大10km的距离，所有光在光纤中沿着同一路径传输，如下图所示。单模链路主要用于长距离传输，应用于Symmetrix Fibre Channel适配器的几个版本。

 

多模链路相对于单模成本较低，用于无需单模那样远距离传输的场景。光纤通道链路通常基于50或62.5µm芯径并支持光波长约为800nm。这种相对于单模增加的芯径意味着光在光纤中有多种传播路径。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIXG7zZcDhKWib5UuZOib1FTfuBMHV37vK904Rs9iaD6PicWlw9j8Rx0ZVibTKgQN59XW7LIGWYRbtIUEsw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

这就导致一种情况：某些频率的光在光纤中沿着一条路径传输而其他光沿着另一条路径，这种结果称为模态色散（Modal Dispersion）。这导致光呈放射状从而限制了多模线缆的距离。

 

**网络（Fabric）：**

 

术语Fabric用于光纤通道描述通用的交换或路由结构，该结构依据帧头的目的地址来传递帧。网络可能是端到端，交换光纤或是仲裁环路。

 

**拓扑（Topology）：**

 

光纤通道拓扑描述端口之间的物理互连。光纤通道支持三种不同的互连方案，称为拓扑结构。分别是点对点，仲裁环和交换结构。















## Fibre Channel光纤通道系统基础（三）

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-25*

光纤通道技术（Fibre Channel）是一种网络存储交换技术，可提供远距离和高带宽，能够在存储器、服务器和客户机节点间实现大型数据文件的传输。了解光纤通道技术是了解网络存储的起点。本系列文章全面介绍光纤通道系统的基础知识。内容包括Fibre Channel基本概念和优势，术语解释，拓扑结构。作为存储爱好者的入门读物。

 

本文作为系列之三，介绍光纤通道技术中的拓扑结构：端到端，光纤交换，仲裁环路，混合四种结构。

 

光纤通道提供了三种不同的拓扑结构和一个混合的互连拓扑结构。这些拓扑结构是：

- 端到端
- 光纤交换
- 仲裁环路
- 混合

 

 

**端到端：**

 

端到端拓扑是所有拓扑结构中最简单的一种，允许两个N_Port通过链路直接互连。各N_Port的发送端直接连至另一端口的接收端。此链路为这两个端口专用，访问链路无需特定协议，因此这两个端口完全占据链路带宽。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWwbH2cJja1rwbuGwh6ibTib3YCSCwzYP7UIAiazuKyo5qsL0BIbVZgh1peTLicfoMk7607nrlmLboGvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**光纤交换：**



端到端拓扑虽然很简单直观，但连接数量有限。这就导致了光纤交换技术的诞生，理论上支持一千六百万个端口（2^24）。交换网络可以包含单个交换机，或多个交换机互连作为一个逻辑整体。

 

每个N_Port通过相关链路连接至光纤网络端口（F_Port）。在光纤网络内各F_Port通过路由功能连接。这就使帧结构按照帧头的目标地址从一个F_Port路由至另一个F_Port。

 

多个并发连接可以同时在N_Port之间共存，因此，随着N_Port数量的增加，聚合带宽也在增长。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWwbH2cJja1rwbuGwh6ibTib3cxOSEBic15DLkowb6mdqFcrxTjo325Dp1GXyg0kmicVflOic2ibkljeJAQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**仲裁环路：**

 

仲裁环路比端到端提供更多连接，可在一个回路上支持126个NL_Port和1个FL_Port，在端到端和光纤交换之间提供一个中间值。在仲裁环路中一个端口的发送输出连接至下一个端口的接收端，所有节点之间都有这样的连接直到形成一个闭合环路。如下图所示。这类配置通常使用光纤通道集线器从而无需使用线缆。仲裁环路中各端口在环路上发现所有消息并忽视/传递目的地非本端口的信息。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWwbH2cJja1rwbuGwh6ibTib3ZIfoGpCEBkhjJ3ibtHwZ1vZBfia3mxibYibGnvdXjdFIacDkwg0JIbg9EA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**混合光纤：**

 

光纤通道通过连接一个活多个仲裁环路到网络从而支持混合拓扑。这种方式结合了两种拓扑的长处。光纤网络拓扑提供连接选择和高聚合带宽，而仲裁环路拓扑提供低成本连接和共享带宽，而无需增加光纤交换机成本。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWwbH2cJja1rwbuGwh6ibTib31ZqJrOzgibPhgX5zerymfkibicXHFDquxeNhn6JMud08qZwE6mGHDOBew/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

混合配置的好处在于仲裁环路上的NL_Port可通过交换机上的FL_Port连接光纤交换机上的N_Port，但需要进行必要的转换。这种转换包括将光纤网络地址转换成环路地址，以及将环路地址转换成光纤交换地址。该配置同时允许N_Port连接至仲裁环路上的NL_Port。



















## InfiniBand技术简介

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-03-30*

   随着CPU和通讯处理速度的不断加快，10Gbps、100Gbps的逐步普及，传统的I/O标准和系统，例如PCI、Ethernet、Fibre Channel可能已经无法跟上脚步。因此如何将旧有的设备或产品升级为高速的通讯系统，正是IT从业者目前普遍苦恼的问题。



   InfiniBand标准（简称IB）的出现就是为了解决PCI等传统I/O架构的通讯传输瓶颈。该标准采用点对点架构，提高容错性和扩展性，在硬件上实现10Gbps的数据传输（每个独立的链路基于四针的2.5Gbps双向连接），采用虚拟通道（Virtual Lane）实现QoS，同时借由CRC技术来保证信号的完整性。本文将介绍InfiniBand这一技术标准，以及它的主要组成部分。

 

**InfiniBand架构:**

 

   InfiniBand采用双队列程序提取技术，使应用程序直接将数据从适配器送入到应用内存（称为远程直接存储器存取或RDMA）, 反之依然。在TCP/IP协议中,来自网卡的数据先拷贝到核心内存，然后再拷贝到应用存储空间，或从应用空间将数据拷贝到核心内存,再经由网卡发送到Internet。这 种I/O操作方式，始终需要经过核心内存的转换，它不仅增加了数据流传输路径的长度，而且大大降低了I/O的访问速度，增加了CPU的负担。而SDP则是将来自网卡的数据直接拷贝到用户的应用空间，从而避免了核心内存参与。这种方式就称为零拷贝，它可以在进行大量数据处理时，达到该协议所能达到的最大的吞吐量。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUCcRXTLDHG91cHrVvicJq7vARoG08dHbGhLhA73llGPg0icVTRJFGcgWiadyqDRRo4hdlTyf5LVibVSQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   InfiniBand的协议采用分层结构，各个层次之间相互独立，下层为上层提供服务。其中物理层定义了在线路上如何将比特信号组成符号，然后再组成帧、数据符号以及包之间的数据填充等，详细说明了构建有效包的信令协议等；链路层定义了数据包的格式以及数据包操作的协议，如流控、 路由选择、编码、解码等；网络层通过在数据包上添加一个40字节的全局的路由报头（Global Route Header, GRH）来进行路由的选择，对数据进行转发。在转发的过程中，路由器仅仅进行可变的CRC校验,这样就保证了端到端的数据传输的完整性；传输层再将数据包传送到某个指定的队列偶（Queue Pair, QP）中，并指示QP如何处理该数据包以及当信息的数据净核部分大于通道的最大传输单元MTU时，对数据进行分段和重组。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUCcRXTLDHG91cHrVvicJq7v2rIt86zWiaSHjicXAujQySAMzaMSoSkRF23Wynsgbrjt28gemuvcO16A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**InfiniBand基本组件：**

 

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUCcRXTLDHG91cHrVvicJq7vQup9Dqj772Tbia6vmJC2aEVBooCUm9uaB9wsM6puJokEicTPVKkAsdUw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   InfiniBand的网络拓扑结构如上所示，其组成单元主要分为四类：

- HCA（Host Channel Adapter），它是连接内存控制器和TCA的桥梁

- TCA(Target Channel Adapter)，它将I/O设备（例如网卡、SCSI控制器）的数字信号打包发送给HCA

- InfiniBand link，它是连接HCA和TCA的光纤，InfiniBand架构允许硬件厂家以1条、4条、12条光纤3种方式连结TCA和HCA

- 交换机和路由器

  

   无论是HCA还是TCA，其实质都是一个主机适配器，它是一个具备一定保护功能的可编程DMA（Direct Memory Access，直接内存存取 ）引擎。

 

**InfiniBand应用：**

 

   在高并发和高性能计算应用场景中，当客户对带宽和时延都有较高的要求时，可以采用IB组网：前端和后端网络均采用IB组网，或前端网络采用10Gb以太网，后端网络采用IB。由于IB具有高带宽、低延时、高可靠以及满足集群无限扩展能力的特点，并采用RDMA技术和专用协议卸载引擎，所以能为存储客户提供足够的带宽和更低的响应时延。



IB目前可以实现以及未来规划的更高带宽工作模式有（以4X模式为例）：

- SRD (Single Data Rate)：单倍数据率，即8Gb/s
- DDR (Double Data Rate)：双倍数据率，即16Gb/s
- QDR (Quad Data Rate)：四倍数据率，即32Gb/s
- FDR (Fourteen Data Rate)：十四倍数据率，56Gb/s
- EDR (Enhanced Data Rate)：100 Gb/s
- HDR (High Data Rate)：200 Gb/s
- NDR (Next Data Rate)：1000 Gb/s+

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIUCcRXTLDHG91cHrVvicJq7vCib8RJqmLGZLDRxTJ8Piaqzsfnl6OnV0vVh1KRckwqP89gSlAgOVMNEA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)























## 新的业务环境对光纤通道的需求——浅谈第六代光纤通道标准

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-15*

   光纤通道行业协会（FCIA）公布了代表业内最快行业标准网络协议的第六代光纤通道（Gen 6 Fibre Channel）协议，它能够使SAN网络的速度达到128G，并具备提供更高网络可靠性、更高能效和更简便操作性的功能。第六代光纤通道协议是旨在提供满足超大规模虚拟化、SSD 存储技术和新数据中心架构的性能、可靠性和可扩展性等需求的下一代光纤通道协议。解决方案预计将在 2016 年广泛上市。



   本文将谈谈新的业务环境对光纤通道的需求，即为什么要推出第六代光纤通道标准。



​    ![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XibV3cY04oDNn5EibxHPcebUIoX97KDD3L5wQ2H73kPiaefS6PLlbeY13w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 

**新的业务环境对光纤通道的需求:**

 

   新的不断变化的关键工作负载、更高密度的虚拟化和基于云的架构继续使SAN基础架构一再突破极限。此外，基于SSD的存储等新技术和新的第六代光纤通道技术存储阵列将重点从存储转移到了互连上。这一趋势要求更高的输入/输出（I/O）和带宽，进而推动着对更高速度和更可靠网络的需求。一系列服务器和存储趋势及技术进步成果使第六代光纤通道技术需求水涨船高，包括：



- **更多、更大的应用**



所有计算环境快速增长，导致软件应用越来越大，数量越来越多。信息数字化、富媒体的日益普及和交互式Web 2.0应用快速发展，推动着对更高存储容量和带宽的要求。从移动接入到电子邮件再到云计算，需要我们在更远的距离上高速传输数据。此外，数据库等应用和其它关键任务应用快速增长，继续要求从不间断的可用性。随着维护窗口日益缩短或彻底消失，会影响数据接入的性能问题或故障停机变得无法容忍。存储网络必须做好准备，提供更高的容量、吞吐量和弹性。



- **高密度服务器虚拟化**

光纤通道刚面世时，一台服务器专用于某一种应用，导致服务器资源利用率很低。服务器虚拟化技术的面世改变了这一局面，允许多种应用共享同一台物理服务器，进而提高了效率和服务器资源利用率。最近的IT支出意向调查显示，连续两年，服务器虚拟化的更广泛普及是IT部门的首要重点之一。今天，不断演进的关键工作负载和Tier 1应用正被托管到虚拟机上。除了服务器虚拟化的更广泛普及外，虚拟机密度（每台物理服务器上托管的虚拟机数量）也稳步增长，达到了每物理服务器10、20甚至更多虚拟机——它们都从SAN中启动并接入SAN资源。虚拟机的使用范围和重要性不断增加，密度不断提高，日益要求存储计算基础架构提供更高的性能（带宽和I/O）、可靠性和可用性。在高度虚拟化环境中，存储网络中的任何拥塞、低劣的I/O性能或故障都会影响到大量应用。



- **固态硬盘（SSD）和闪存**

闪存和SSD的面世使存储技术突飞猛进，大大缩短了服务器侧（使用多核处理器和更快速的内存）与存储侧之间由来已久的I/O性能差距。SSD存储可化解I/O和吞吐量瓶颈，为高密度虚拟化工作负载和传统的关键任务应用提供更快速的数据块和文件存储性能。然而，这以存储网络能跟上发展步伐为前提。不管是部署为基于SSD的独立阵列还是直接连接到服务器CPU和内存总线，SSD均可提高I/O性能，更加需要存储网络提供更高的I/O带宽性能和更高的可用性。

 



**第六代光纤通道标准：**

 

   第六代光纤通道协议是旨在提供满足超大规模虚拟化、SSD 存储技术和新数据中心架构的性能、可靠性和可扩展性等需求的下一代光纤通道协议。它包括了下面六个新特性：



1. 32GFC至128GFC的数据吞吐量：第六代光纤通道协议将3200 MB/s的16GFC数据吞吐量翻了一番，达到32GFC，这实现了6400 MB/s的全双工速度。第六代光纤通道协议还提供“将32 GFC翻两番，达到128GFC”的选项，从而实现基于光纤通道的无缝兼容和向下兼容技术的25600 MB/s的全双工速度。
2. 向前纠错（FEC）：通过自动检测以及发生于高速网络的位错误恢复功能来提高链路可靠性。FEC有助于尽量减少或避免可导致应用程序性能下降或中断的数据流错误。
3. 能效：通过允许光纤通道光连接器在待机模式（或“休眠”模式）每秒多次运行而实现较低的能耗。
4. NPIV（N-Port ID Virtualization）支持：NPIV技术可以简化服务器虚拟化的部署，并且支持扩展至超大型的SAN网络。
5. 向后兼容性：128G和32G光纤通道可支持全面、彻底向后兼容16G和8G网络。为确保完整的投资保护，第六代光纤通道技术可在任何两个网络点之间自动配置到最快支持速度，并且不需要任何用户干预。
6. 加强的安全性：第六代光纤标准采用了许多知名的安全和保护标准，如FC-SP、FC-SP-2和FC-SP-2/AM1。这使得它仍然支持包括美国国家标准技术研究所（NIST）和欧盟（EU）在内的国际组织的标准。





















## 以太网矩阵(Ethernet Fabric)简介

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-12*

   数据中心网络依赖于以太网。在过去几十年中，伴随着新应用架构类型的不断涌现，以太网也在不断地发展变化。今天，数据中心网络负责传输多种应用的流量，包括客户端/服务器、Web服务、统一通信、虚拟机和存储流量。每种应用的流量模式和网络设备要求均有所不同。应用被大量地部署在在数据中心托管的服务器集群中的虚拟机上。以太网可用于构建共享存储池，这就对网络提出了苛刻要求，包括无损的数据包交付、确定性的延时和更高的带宽等。这些变化共同推动着以太网技术的下一轮发展：即以太网矩阵。



   本文将介绍什么是以太网矩阵，以及它的优势所在。

 

**传统以太网:**

 

   为了更好地了解以太网矩阵，让我们先看看传统以太网。与单一以太网交换机提供的端口数相比，大多数数据中心需要更多端口，因此数据中心纷纷将多台交换机连接起来，形成一个可支持更多连接的网络。例如，服务器机架通常包括一台架顶式（ToR）交换机和多个服务器，多个这样的机架又连接到架端（EoR）交换机。所有这些以太网交换机都互相连接起来，形成一种分层或“以太网树状”拓扑，如下图所示。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016X1fr2sfE65ooMneSvwnDuR2tdZzatfYALDqIuRhreBSDzpqTho2XlTw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



   在传统以太网中，交换机之间的连接，或者说交换机间互联链路（ISL，图中蓝色线条所示），不允许形成环路，否则帧就无法正常传输。生成树协议（STP）可创建一个任意两台交换机之间只有一条活动路径的树状拓扑，因此可以防止形成环路。（在图中，非活动路径用虚线表示。）这意味着ISL带宽仅限于单一逻辑连接，因为交换机间不允许建立多条连接。对以太网的增强试图突破这一局限性。业内开发了Link Aggregation Groups（LAG），将交换机间的多条链路作为单一连接进行处理而不形成环路。但是，LAG必须在LAG中的每个端口上手工进行配置，因此缺乏灵活性。



   树状拓扑要求流量向树上方或下方（或者说“南北向（north-south）”）传输，才能达到相邻机架。在大多数接入流量在机架内不同服务器间传输时，这不成问题。但在服务器集群，如集群应用和服务器虚拟化所需的服务器集群中，流量在多个机架内的不同服务器之间传输，也就是沿“东西向（east-west）”传输，因此树状拓扑会由于存在多个中继段而增加延时，而且由于不同交换机之间只有一条链路而限制带宽。



   某条链路丢失时，STP可自动恢复。然而，这会停止网络中的所有流量传输，而且必须在网络中的所有交换机之间重新收敛到一条路径才能重新开始流量传输。使所有链路上的所有流量传输停止数十秒甚至几分钟，会限制可扩展性，并将流量大量发送到可容忍数据路径阻塞以确保链路弹性的应用中。过去，流量依靠TCP来应对这种业务中断，但是今天，几乎所有数据中心应用都以24 x 7高可用性模式运行，而且以太网中的存储流量不断增加，因此数据路径哪怕中断几秒钟都是不可接受的。



   最后，传统以太网交换架构还会带来其它限制。每台交换机都有自己的控制平面和管理平面。每个帧达到某个入站端口时，每台交换机都必须发现并处理每个帧的协议。随着更多交换机的添加，协议处理时间会导致延时不断增加。每台交换机和交换机中的每个端口都必须独立进行配置，因为不同交换机之间根本不共享通用配置和策略信息。复杂性不断提高，配置错误不断增加，而运营和管理资源却不能相应增加。

 



**以太网矩阵架构：**

 

   下图显示了传统以太网交换机的架构。控制平面、数据平面和管理平面从逻辑上通过背板（back plane）连接到每个端口上。控制和管理平面在交换机一级运行而不是在网络级运行。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XKLU3DaDA3vGxFlibHY2YqGLiaMFSRbaAoiczEPiaAPgowzhSDjGic3xDt3Q/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



   以太网矩阵可以看作是将控制和管理平面从物理交换机扩展到Fabric架构中。如下图所示，它们现在在Fabric架构一级运行而不是在交换机一级运行。



![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWuvwibrrxmGa68yfLPib016XSQmH4ar4f9kIxKh0sIXkf2YMLSTjibtDKdw4TCWAL6qW2WKJOeTJBwQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



   在以太网矩阵中，控制平面采用链路状态路由代替STP，而数据路径在2层提供等价多路径转发，使数据始终可通过最短路径传输，使用多条ISL连接而不形成环路。与Fabric架构的控制平面相结合时，带宽扩展变得简单无比。例如在一台新交换机连接到Fabric架构中的任何其它交换机时，就可以自动形成新的捆绑链路。如果某捆绑链路出现故障或被删除，流量可以重新均衡到现有链路儿不中断业务。最后，如果一条ISL被添加到Fabric架构中或从中删除，其它ISL上的流量可继续传输而不会像使用STP时那样停止。



   在这种架构中，一组交换机可定义为“逻辑机箱”的一部分，就像机箱式交换机中的接口板卡一样。这样就可以简化管理、监控和运行，因为策略和安全配置参数可在逻辑机箱中的所有交换机之间轻松共享。此外，与物理和虚拟服务器及存储设备的连接的信息并不发送给Fabric架构中的所有交换机，因此Fabric架构可确保所有网络策略及安全设置可继续用于任何给定的虚拟机，不管它是否移动，也不管它位于何处。

 



**以太网矩阵的优势：**

 

   与传统的分层以太网架构相比，以太网矩阵可提供更高的性能、利用率、可用性和操作简便性。至少，它们具有以下特征：



​    **更扁平**：以太网矩阵不再需要生成树协议（STP），但仍可与现有的以太网全面互操作。

​    **灵活**：可采用任何拓扑结构，来最有效地满足各种工作负载的需求

​    **有弹性**：使用多条“最低成本”路径来确保高性能和高可靠性

​    **可扩展**：可根据需要轻松地向上或向下扩展。



   更先进的以太网矩阵从光纤通道Fabric架构设计中借鉴了更多：

- 它们可以自动形成，作为单一逻辑实体运行，其中的所有交换机都能自动识别彼此的存在，而且知道所有相连接的物理和逻辑设备。
- 管理可基于域而不是基于设备，并通过策略而不是重复的流程进行定义。
- 这些特性，再加上专门针对虚拟化进行的增强，可帮助更轻松地应对网络中的虚拟机自动化挑战，从而实现更高程度的IT自动化。
- 协议融合（如Fibre Channel over Ethernet，FCoE）也可以是更好地将LAN和SAN流量桥接起来的一个特性。





















## 浅谈SDN和NFV的区别

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-16*

   EMC之前宣布成立新的NFV Group（Network Functions Virtualization Technology Group），并由联邦的核心智库CTO Office来直辖，EMC CTO Office的负责人John Roese在EMC PULSE博客中表示，EMC成立新的NFV Group，该部门成立的用意很明确，帮助运营商转型，助力它们迎接更为广阔的市场机遇。



   那到底什么是NFV（网络功能虚拟化），它和之前的SDN（Software-defined Networking）软件定义网络概念是一回事吗？它们有什么区别？本文就带大家来一探究竟。

 

**SDN-诞生于校园，成熟于数据中心:**

 

   SDN初始于园区网络，一群研究者（斯坦福的达人们）在进行科研时发现，每次进行新的协议部署尝试时，都需要改变网络设备的软件，这让他们非常郁闷，于是乎，他们开始考虑让这些网络硬件设备可编程化，并且可以被集中的一个盒子所管理和控制，就这样，诞生了当今SDN的基本定义和元素

- 分离控制和转发的功能
- 控制集中化（或集中化的控制平面）
- 使用广泛定义的（软件）接口使得网络可以执行程序化行为



   另一个SDN成功的环境就是云数据中心了，这些数据中心的范围和规模的扩展，如何控制虚拟机的爆炸式增长，如何用更好的方式连接和控制这些虚拟机，成为数据中心明确需求。而SDN的思想，恰恰提供了一个希望：数据中心可以如何更可控。

 

**OpenFlow–驱动向前的标准：**

 

   那么，OpenFlow是从何处走进SDN的视野中呢？当SDN初创伊始，如果需要获得更多的认可，就意味着标准化这类工作必不可少。于是，各路公司联合起来组建了开放网络论坛（ONF），其目的就是要将控制器和网络设备（也就是SDN提到的控制平面和转发平面）之间的通讯协议标准化，这就是OpenFlow。OpenFlow第一定义了流量数据如何组织成流的形式，第二定义了这些流如何按需控制。这是让业界认识到SDN益处的关键一步。

 

**NFV-由运营商提出：**

 

   和SDN始于研究者和数据中心不同，NFV则是由运营商的联盟提出，原始的NFV白皮书描述了他们遇到的问题，以及初步的解决方案。



   运营商网络的设备呈指数级的增长，越来越多各种类型的硬件设备不断的增加。当开展一个新的网络业务时，往往提出多样化的需求，寻找适合空间和电力去容纳这些“盒子”变得越来越困难。能耗的增加，资本投资的挑战，以及设计，集成和运行这些日益复杂的基于硬件的平台所需要的技术这些种种挑战复合在一起。另外，基于硬件平台的很快就有可能到达其生命周期，需要重复大量的采购–设计–集成–部署周期，也只能获取少量利润收益，甚至可能没有收益。



   网络功能虚拟化的目标是使用标准的IT虚拟化技术，把现在大量的位于数据中心，网络节点以及最终用户处的这些不同类型网络设备–标准的服务器，交换机和存储设备集合在一起。我们相信网络功能虚拟化可以适用于任何数据平面的包处理，控制平面的功能集成，以及无线网络的基础架构中。

 

**SDN vs NFV：**

 

   现在，让我们看看SDN和NFV的关系，原始的NFV白皮书给出一个SDN和NFV关系的概述



   如图所示，网络功能虚拟化和软件定义网络有很强的互补性，但是并不相互依赖（反之亦然），网络功能虚拟化可以不依赖于SDN部署，尽管两个概念和解决方案可以融合，并且潜在形成更大的价值。

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWPxum8J4iaSSW8ibIa5EnRZx8RgUbz93SicATyGh2MD5ia8Ty4ULL8zlyiaU2jZseIWlE5mN0B3wicdreA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   依赖于应用在大量数据中心内的现有技术，网络功能虚拟化的目标可以基于非SDN的机制而实现。但是，如果可以逐渐接近SDN所提出的将控制平面和数据平面的思路，那么就能进一步使现有的部署性能增强且简化互操作性，减轻运营和维护流程的负担。网络功能虚拟化为SDN软件的运行提供基础架构的支持，未来，网络功能虚拟化可以和SDN的目标紧密联系在一起—-使用通用的商业性服务器和交换机。

 

**SDN和NFV协同工作？：**

 

   让我们看一个SDN和NFV协同工作的案例，首先，下图展示了当今路由器服务部署典型案例，在每个客户站点使用均使用一台路由器提供服务：

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWPxum8J4iaSSW8ibIa5EnRZxA7nguIoFkMSJmUIicdicqlm0w086qhjVvJDcqgUrEg9wBUGt5ibTQTEAg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   如下图所示，使用虚拟路由器的功能，NFV就可以在这个场景中展现作用，所有的用户站点左侧都是一个网络接口设备（NID）–虚拟路由器，提供网络的分界点，并且测量性能：

![图片](http://mmbiz.qpic.cn/mmbiz/TztEwAzAQIWPxum8J4iaSSW8ibIa5EnRZx6WKTYTR7PtLsnjyKIefGYUzr9rIov8jNbKtPwgmATwM9l9TIxAmFkw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   最终，SDN被引入进来，将控制平面和转发平面分割，数据包将会根据更优化的数据平面被转发，路由（控制平面）功能则运行在某机柜服务器的虚拟机内。



   SDN和NFV的结合提供了最优的解决方案

- 一个昂贵的专业设备被通用硬件和高级软件替代
- 软件控制平面被转移到了更优化的位置（从专用设备硬件中剥离，放置在数据中心或者POP位置，可能以服务器或者虚拟机的形式存在）
- 数据平面的控制被从专有设备上提取出来，并且标准化，使得网络和应用的革新无需网络设备硬件升级

 

**汇总：**

 

   下表列举了SDN和NFV的一些关键点比较

​         

| 分类       | SDN                                  | NFV                                            |
| ---------- | ------------------------------------ | ---------------------------------------------- |
| 产生原因   | 分离控制和数据平面中央控制可编程网络 | 从专有硬件到普遍硬件过渡重新定位网络功能       |
| 目标位置   | 校园网络，数据中心/云                | 运营商网络                                     |
| 目标设备   | 商用服务器和交换机                   | 商用服务器和交换机                             |
| 初始化应用 | 基于云协调器和网络                   | 路由器，防火墙，网关，CDN，广域网加速，SLA保证 |
| 新的协议   | OpenFlow                             | 尚无                                           |
| 组织者     | Open Networking  Forum (ONF)         | ETSI NFV Working  Group                        |

 

















## 工欲善其事，必先利其器 – 网络抓包

原创 EMC中文技术社区 [戴尔易安信技术支持](javascript:void(0);) *2016-04-17*

   其实抓包不是难事，很多人都会，下个Protocol Analyzer，比如Network Monitor、Wireshark，打开点记下就可以抓了。我主要有三个问题：

1. 你为什么想都要抓包？你的问题是否能通过抓包获取答案？
2. 在什么情况下抓包可以帮助你发现答案？
3. 如潮水般涌入的网络包瞬间就能淹没Protocol Analyzer的Packet List，你要从何下手？



今天就简单来谈一下这几个问题

 

   抓包可使用的场景很多，排错、验证、测试、核对等等，我就举几个例子来说明吧。



**Case I**：客户在一台存储上启用了SNMP服务，随后想通过验证UDP161/162的侦听状态来确认服务是否确实启动了。



详情I：最简单的方式就是用进入存储的OS运行类似netstat –anop查看UDP端口状态，还有些同学会条件反射的说telnet一下呗。客户玩得比较高级，用一个叫做nmap的工具做端口扫描，发现扫描结果是Open | Filtered，这又算哪门子意思？除非找到这些工具的使用说明，否则无法明白其输出的意思。但有的时候就是没有太多参考文档，比如netstat显示的UDP端口状态列都是空的，没有任何状态，这代表什么呢？其实网络抓包可以帮助我们。你可以在telnet的同时抓包，结果会发现telnet发起了TCP SYN包，立刻就被对端给Reset掉了，所以用telnet的提议是错误的，用TCP去验证一个UDP端口是否在侦听，显然是南辕北辙了。同样，做nmap扫描的同时抓包，你会发现原来UDP使用ICMP返回消息来判断端口状态。如果端口关闭，那么对端会返回port unreachable，如果没有任何ICMP返回呢？那就说明中间存在包过滤逻辑，这也是为什么客户的nmap扫描显示open | filtered，nmap也无法确认端口状态，因为没有任何ICMP消息返回。所以，客户的扫描结果不能判断端口状态，必须采取其它手段。不过这不是这个case的重点，我们的重点是可以通过抓包看到应用程序的行为。

总结：判断端口状态的方式很多，但你是否采用了正确的方式，完全可以通过抓包来判断，它会告诉你一个应用程序的行为，帮助你发现原因。

 



**Case II**：客户发现应用程序与服务器之间的通信很慢。



详情II：为什么拿这个case来说，因为抓包和对TCP的分析在这个case里几乎就直接问题原因所在了。通过分析TCP分段，我们发现了traffic pattern的变化，延迟由正常的几个ms逐渐转变为了十几个ms，并在之后的流量中看到了Window Full以及最终的ZeroWindow消息。对TCP熟悉的同学立马能够判断出接收端存在处理能力的问题，导致无法及时清理TCP接收缓存，使得队列长度越来越长，根据Little Law和Utilization Law，响应时间会呈指数上升，这也为什么我们会观察到响应时间的变化。还有同学提问中间的交换机/路由器是否有可能发生这样的过载？当然是肯定的，但我们这个case里不是这个问题，为什么？因为我们抓包没有看到任何重传。

总结：抓包在这个case帮你排除了看起来最有可能是网络问题的问题，TCP的行为非常清晰的指出了问题所在。

 



**Case III**：Http下载失败



详情III：抓包下载过程，发现下载失败是因为TCP reset，但又是什么导致TCP reset呢？客户是一家web hosting网站，提供文件下载，他说这不是个案，有很多他们的客户抱怨说下载失败。因此我们初步会怀疑是服务器端的问题。在抓包后，我们发现数据传输开始是正常的，数据包长度都为1460bytes，而且客户端也在不时的Ack数据。直到某个数据没有被Ack，随后变发现了tcp reset。仔细观察后发现，没有被Ack的那个包的长度并非1460bytes，而其它所有数据包都是1460bytes。至此，虽然不能100%确认问题所在，但是你有理由让服务器端的应用开发人员做Application debugging，为什么服务器端应用程序会改变发包大小，当然对TCP来讲，这不是问题，比如TCP sender buffer就是只有这么点数据。但突然之间的大小变化，以及Ack的奇怪行为让我们有理由怀疑，服务器端程序的逻辑或许存在问题。

总结：在海量数据下，如何做到火眼晶晶找出数据包的不同是需要练习的。

 



**Case IV**：这并非是一个问题，还是要教会我们如何怀疑任何存在疑点的延迟。



详情 IV：理论上，你应该很清楚自己网络内所有的delay component，因为它们都是可以通过计算获得的。但有的时候，无法解释的延迟，就需要你理解协议和看包的能力。在这个case中，我们遇到的是TCP Nagle和Delayed-Ack算法之间的临时通信死锁问题。Nagle只允许一次发送一个小包（<MSS），除非收到Ack；而Delayed Ack要求至少收到两个包，否则不Ack；如此一来，两种就会较劲，直至Delayed-Ack timer超时，通常会有200ms的延迟。因此，在这种情况下，对延迟非常敏感的应用很容易超时报错，甚至crash。你完全可以抓包，而理解TCP行为和解释不正当的200ms延迟就是你的任务了。通常来说，在如今的一个健康的LAN内，不可能会有200ms网络延迟，一定是协议或应用本身的行为，而抓包给你机会去证明这一切。

 



**Case V**：NFS mount失败



详情V：这是我自己的一个case，我并不熟悉NFS，只是在测试VNX时，NFS mount失败。抓包后发现整个MOUNT过程涉及了RPC、port-mapper、NFS、MOUNT四种上层协议。于是我就下载了所有RFC扫了一遍，发现协议消息本身的数据结构和消息编号在RFC文档内都能找到。对于我见到的错误消息，找到对应的报错协议RFC并搜索该错误，立马就能找到对应的解释，原来是权限问题。到VNX上调了一下，问题就解决了。

总结：抓包可以帮住你揭示一些隐藏在上层协议下面的东西，你以为自己只是在做nfs mount，但其实涉及了不同的协议，任何一个出现问题，都会发生失败。抓包显示的错误消息，帮助你知道应该搜索哪个文档，哪个关键字，直指问题所在。这也有助于你学习新的协议。

 

   最后，给到大家一些建议：



- 网络包在大部分时候能够告诉你真相，所以用好它很有价值。
- 网络包告诉你发生了什么，但不会告诉你为什么发生，理解协议行为和分析是你的任务。
- 不要只看文档，尝试抓一下包，因为真相可能和文档是不同的。
- 面对海量数据包，需要训练你的眼睛和大脑来过滤消息
- TCP其实是老好人，不要总是责怪它。理解它，分析它，你会发现错误大部分时候是在别的地方。
- 新一代数据中心网络十分复杂，用好工具，会帮你解决很多难题。